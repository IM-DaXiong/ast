# -*- coding: utf-8 -*-
import os
import json
import shutil
import re
import datetime
import torch
from tqdm import tqdm
import numpy as np
import soundfile as sf
import io
import sys
import requests
import math
import time

# --- æ ¸å¿ƒä¿®å¤ï¼šå¼ºåˆ¶æ§åˆ¶å°ä½¿ç”¨ UTF-8 ---
os.environ["PYTHONIOENCODING"] = "utf-8"
os.environ["PYTHONUTF8"] = "1"
os.environ["HF_HOME"] = r"L:\Models\huggingface"  # è®¾ç½®HFç¼“å­˜è·¯å¾„

if sys.stdout.encoding != 'utf-8':
    try:
        # Python 3.7+ çš„æ ‡å‡†ä¿®å¤æ–¹æ³•
        sys.stdout.reconfigure(encoding='utf-8', errors='replace')
        sys.stderr.reconfigure(encoding='utf-8', errors='replace')
    except AttributeError:
        # å…¼å®¹æ—§ç‰ˆæœ¬
        sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8', errors='replace')
        sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8', errors='replace')


# --- é…ç½®åŒº ---
SOURCE_DIRS = [
                # r"D:\2024-ç½‘ç»œèµ„æº\ä¸­æ–‡éŸ³æ•ˆ",
                # r"E:\sounds\1Archetypeä½æ²‰æ’å‡»",
                # r"E:\sounds\3Enduranceä½éŸ³ç´§å¼ ",
                # r"E:\sounds\ã€34ã€‘å˜å½¢é‡‘åˆšç§‘å¹»éŸ³æ•ˆåˆè¾‘",
                # r"E:\sounds\4Idyllicä¼˜é›…èˆ’ç¼“",
                # r"E:\sounds\6Beginningsæ·±æ²‰å™äº‹é…ä¹",
                # r"E:\sounds\AI\5000+å¸¸ç”¨éŸ³æ•ˆ",
                # r"E:\sounds\Boom&Rain\Boom_Library_Thunder_Rain_SURR",
                # r"E:\sounds\å•ç‹¬ä¸‹è½½",
                # r"D:\2024-ç½‘ç»œèµ„æº\ä¸­æ–‡éŸ³æ•ˆ",
                #r"E:\sounds\éŸ³æ•ˆç´ æ1",
                #r"E:\sounds\éŸ³æ•ˆç´ æ2",
                #r"E:\sounds\éŸ³æ•ˆç´ æ3",
                #r"E:\sounds\ã€86ã€‘40G Audiojungleèµ„æºåº“",
                #r"E:\sounds\ã€39ã€‘Bluezoneå…¬å¸å‡ºå“çš„å„ç±»éŸ³æ•ˆ",
                #r"E:\sounds\ã€39ã€‘Bluezoneå…¬å¸å‡ºå“çš„å„ç±»éŸ³æ•ˆ",
                #r"E:\sounds\å‘¼å¸",
                #r"E:\sounds\å‘¼å¸",
                #r"E:\sounds\å•ç‹¬ä¸‹è½½",
                # r"L:\BBC-SFX\BBCSoundEffectsComplete\6-30",
                # r"L:\BBC-SFX\BBCSoundEffectsComplete\A-C",
                r"L:\BBC-SFX\BBCSoundEffectsComplete\sounds",
                # r"E:\sounds\æˆ˜äº‰&ç§‘æŠ€\Bigfilms CHAOS - Sound FX",
              ]
    # è·å–éŸ³é¢‘æ–‡ä»¶
audio_exts = ('.wav', '.mp3', '.flac', '.ogg', '.aiff', '.m4a')
#audio_exts = ('.mp3')
TARGET_DIR = r"L:\AI\AI_SFX_BBC_Output"
MODELS_ROOT = r"L:\Models"
HF_CACHE_DIR = os.path.join(MODELS_ROOT, "huggingface")
AST_MODEL = "MIT/ast-finetuned-audioset-10-10-0.4593"
JSON_DB_PATH = os.path.join(TARGET_DIR, "audio_library_v2.json")
OLLAMA_API = "http://localhost:11434/api/generate"
QWEN_MODEL = "qwen:14b-chat-q4_0"

# ã€ä¿æŒå½“å‰éŸ³æ•ˆåˆ†ç±»ä½“ç³»ã€‘
CATEGORY_LIST = [
    "è‡ªç„¶ç¯å¢ƒ",           # æ‰€æœ‰è‡ªç„¶å£°éŸ³ç»Ÿä¸€å½’ç±»
    "åŸå¸‚ç¯å¢ƒ",           # æ‰€æœ‰äººé€ ç¯å¢ƒå£°éŸ³
    "æœºæ¢°è®¾å¤‡",           # å¼•æ“/ç”µæœº/å·¥ä¸šè£…ç½®
    "ç”Ÿæ´»å®¶å±…",           # å±…å®¶æ—¥å¸¸éŸ³æ•ˆï¼ˆæ¦¨æ±æœº/é©¬æ¡¶/åˆ·ç‰™ç­‰ï¼‰
    "äººå£°",               # æ‰€æœ‰è¯­éŸ³/éè¯­è¨€äººå£°
    "åŠ¨ç‰©å£°éŸ³",           # é‡ç”ŸåŠ¨ç‰©/æ˜†è™«/é¸Ÿç±»
    "å†·å…µå™¨",             # åˆ€å‰‘/æš—å™¨/æ ¼æ–—éŸ³æ•ˆ
    "çƒ­å…µå™¨",             # æªæ¢°/çˆ†ç‚¸/ç°ä»£æ­¦å™¨
    "UIäº¤äº’",             # ç³»ç»Ÿ/ç•Œé¢/åé¦ˆéŸ³
    "æŠ½è±¡éŸ³æ•ˆ",           # éç°å®/ç”µå­/å˜å½¢å£°
    "è½¬åœºéŸ³æ•ˆ",           # åœºæ™¯åˆ‡æ¢/è¿‡æ¸¡éŸ³æ•ˆ
    "ç”µå½±æ°›å›´",           # å¿ƒç†/æƒ…ç»ªé“ºå«
    "ç‰¹æ®Šæ•ˆæœ",           # æ—¶ç©º/ç©ºé—´/è¶…è‡ªç„¶
    "æœªåˆ†ç±»ç´ æ"          # å¾…äººå·¥å®¡æ ¸
]

# ã€ä¸“ä¸šå…³é”®è¯åº“ã€‘- ç”¨äºæ–‡ä»¶ååˆ†æï¼ˆå®Œæ•´å½±è§†æ ‡å‡†ç‰ˆï¼‰
PROFESSIONAL_KEYWORDS = {
    # ã€è‡ªç„¶ç¯å¢ƒã€‘
    "è‡ªç„¶ç¯å¢ƒ": [
        # å¤©æ°”
        "rain", "storm", "wind", "thunder", "lightning", "snow", "hail", "fog", "mist", "drizzle", 
        "blizzard", "hurricane", "typhoon", "tornado", "gale",
        "é›¨", "é£", "é›·", "ç”µ", "é›ª", "é›¹", "é›¾", "éœœ", "æ¯›æ¯›é›¨", "æš´é›¨", 
        "æš´é£é›ª", "å°é£", "é¾™å·é£", "é£“é£", "ç‹‚é£",
        
        # æ°´ä½“
        "ocean", "wave", "sea", "tide", "current", "river", "stream", "brook", "creek", "waterfall",
        "rapids", "drip", "splash", "pour", "flood", "ice", "glacier", "underwater",
        "æµ·", "æµª", "æ½®", "æµ", "æ²³", "æºª", "å°æºª", "ç€‘å¸ƒ", "æ€¥æµ", 
        "æ»´æ°´", "æ°´èŠ±", "å€¾å€’", "æ´ªæ°´", "å†°", "å†°å·", "æ°´ä¸‹",
        
        # åœ°å½¢ä¸ç”Ÿç‰©
        "forest", "jungle", "desert", "mountain", "cave", "valley", "plateau", "meadow", "grassland",
        "bird", "animal", "insect", "cricket", "frog", "wolf", "lion", "elephant", "bear",
        "æ£®æ—", "ä¸›æ—", "æ²™æ¼ ", "å±±", "æ´ç©´", "å±±è°·", "é«˜åŸ", "è‰åœ°", "è‰åŸ",
        "é¸Ÿ", "åŠ¨ç‰©", "æ˜†è™«", "èŸ‹èŸ€", "é’è›™", "ç‹¼", "ç‹®å­", "å¤§è±¡", "ç†Š"
    ],
    
    # ã€åŸå¸‚ç¯å¢ƒã€‘
    "åŸå¸‚ç¯å¢ƒ": [
        # äº¤é€šå·¥å…·
        "traffic", "car", "bus", "train", "subway", "metro", "tram", "taxi", "truck", "motorcycle",
        "ambulance", "police_car", "fire_truck", "helicopter", "airplane", "jet", "siren", "horn",
        "äº¤é€š", "æ±½è½¦", "å·´å£«", "ç«è½¦", "åœ°é“", "ç”µè½¦", "å‡ºç§Ÿè½¦", "å¡è½¦", "æ‘©æ‰˜è½¦",
        "æ•‘æŠ¤è½¦", "è­¦è½¦", "æ¶ˆé˜²è½¦", "ç›´å‡æœº", "é£æœº", "å–·æ°”æœº", "è­¦ç¬›", "å–‡å­",
        
        # äººç¾¤ä¸å»ºç­‘
        "crowd", "street", "urban", "city", "downtown", "pedestrian", "market", "shopping", "construction",
        "building", "skyscraper", "bridge", "tunnel", "elevator", "escalator", "staircase", "hallway",
        "äººç¾¤", "è¡—é“", "åŸå¸‚", "éƒ½å¸‚", "å¸‚ä¸­å¿ƒ", "è¡Œäºº", "å¸‚åœº", "è´­ç‰©", "æ–½å·¥",
        "å»ºç­‘", "æ‘©å¤©å¤§æ¥¼", "æ¡¥", "éš§é“", "ç”µæ¢¯", "æ‰¶æ¢¯", "æ¥¼æ¢¯", "èµ°å»Š"
    ],
    
    # ã€æœºæ¢°è®¾å¤‡ã€‘
    "æœºæ¢°è®¾å¤‡": [
        # åŠ¨åŠ›ç³»ç»Ÿ
        "engine", "motor", "turbine", "generator", "compressor", "pump", "fan", "ventilator", "propeller",
        "diesel", "gasoline", "electric", "hydraulic", "pneumatic", "steam", "boiler", "furnace",
        "å¼•æ“", "ç”µæœº", "æ¶¡è½®", "å‘ç”µæœº", "å‹ç¼©æœº", "æ³µ", "é£æ‰‡", "é€šé£æœº", "èºæ—‹æ¡¨",
        "æŸ´æ²¹", "æ±½æ²¹", "ç”µåŠ›", "æ¶²å‹", "æ°”åŠ¨", "è’¸æ±½", "é”…ç‚‰", "ç†”ç‚‰",
        
        # è¿åŠ¨éƒ¨ä»¶
        "gear", "bearing", "chain", "belt", "piston", "crank", "lever", "valve", "sprocket", "rotor",
        "vibration", "grinding", "drilling", "sawing", "cutting", "hammering", "stamping", "pressing",
        "é½¿è½®", "è½´æ‰¿", "é“¾æ¡", "çš®å¸¦", "æ´»å¡", "æ›²è½´", "æ æ†", "é˜€é—¨", "é“¾è½®", "è½¬å­",
        "æŒ¯åŠ¨", "ç ”ç£¨", "é’»å­”", "é”¯åˆ‡", "åˆ‡å‰²", "é”¤å‡»", "å†²å‹", "å‹åˆ¶"
    ],
    
    # ã€ç”Ÿæ´»å®¶å±…ã€‘ï¼ˆæ–°å¢å®Œæ•´ç‰ˆï¼‰
    "ç”Ÿæ´»å®¶å±…": [
        # å¨æˆ¿ç”µå™¨
        "juicer", "blender", "mixer", "food_processor", "kitchen_appliance", "refrigerator", "freezer",
        "microwave", "oven", "stove", "cooker", "dishwasher", "sink", "faucet", "tap", "kettle",
        "æ¦¨æ±æœº", "æ…æ‹Œæœº", "æ–™ç†æœº", "å¨æˆ¿ç”µå™¨", "å†°ç®±", "å†·å†»", "å¾®æ³¢ç‚‰", "çƒ¤ç®±", "ç‚‰ç¶", 
        "ç‚Šå…·", "æ´—ç¢—æœº", "æ°´æ§½", "æ°´é¾™å¤´", "æ°´é˜€", "æ°´å£¶",
        
        # å«æµ´è®¾å¤‡
        "toilet", "flush", "bathroom", "shower", "bathtub", "drain", "plumbing", "running_water",
        "toothbrush", "brushing", "shaver", "razor", "hair_dryer", "comb", "mirror", "sink",
        "é©¬æ¡¶", "å†²æ°´", "å«ç”Ÿé—´", "æ·‹æµ´", "æµ´ç¼¸", "æ’æ°´", "ç®¡é“", "æµæ°´",
        "ç‰™åˆ·", "åˆ·ç‰™", "å‰ƒé¡»åˆ€", "åˆ®èƒ¡åˆ€", "å¹é£æœº", "æ¢³å­", "é•œå­", "æ´—æ‰‹æ± ",
        
        # å®¶å±…ç¯å¢ƒ
        "door", "door_creak", "window", "lock", "key", "clock", "alarm_clock", "phone", "telephone",
        "doorbell", "vacuum_cleaner", "washing_machine", "dryer", "bed", "bed_spring", "snoring",
        "coughing", "sneezing", "footsteps", "floorboard", "creak", "furniture", "chair", "table",
        "é—¨", "é—¨å±å‘€", "çª—", "é”", "é’¥åŒ™", "æ—¶é’Ÿ", "é—¹é’Ÿ", "ç”µè¯", 
        "é—¨é“ƒ", "å¸å°˜å™¨", "æ´—è¡£æœº", "çƒ˜å¹²æœº", "åºŠ", "åºŠå¼¹ç°§", "æ‰“é¼¾",
        "å’³å—½", "æ‰“å–·åš", "è„šæ­¥å£°", "åœ°æ¿", "å±å‘€å£°", "å®¶å…·", "æ¤…å­", "æ¡Œå­"
    ],
    
    # ã€çƒ­å…µå™¨ã€‘ï¼ˆæˆ˜äº‰/æªæ¢°/çˆ†ç‚¸ï¼‰
    "çƒ­å…µå™¨": [
        # æªæ¢°
        "gun", "rifle", "pistol", "shotgun", "machine_gun", "sniper", "bullet", "ammo", "cartridge",
        "firearm", "trigger", "recoil", "reload", "cock", "safety", "silencer", "suppressor",
        "æª", "æ­¥æª", "æ‰‹æª", "éœ°å¼¹æª", "æœºæª", "ç‹™å‡»æª", "å­å¼¹", "å¼¹è¯", "å¼¹åŒ£",
        "ç«å™¨", "æ‰³æœº", "åååŠ›", "è£…å¼¹", "ä¸Šè†›", "ä¿é™©", "æ¶ˆéŸ³å™¨", "æŠ‘åˆ¶å™¨",
        
        # çˆ†ç‚¸
        "explosion", "bomb", "grenade", "mine", "rocket", "missile", "detonate", "blast", "shockwave",
        "fireball", "debris", "shrapnel", "mushroom_cloud", "booming", "thundering", "rumbling",
        "çˆ†ç‚¸", "ç‚¸å¼¹", "æ‰‹æ¦´å¼¹", "åœ°é›·", "ç«ç®­", "å¯¼å¼¹", "å¼•çˆ†", "å†²å‡»æ³¢",
        "ç«çƒ", "ç¢ç‰‡", "å¼¹ç‰‡", "è˜‘è‡äº‘", "è½°é¸£", "é›·å£°", "éš†éš†å£°",
        
        # æˆ˜äº‰åœºæ™¯
        "war", "battle", "combat", "military", "soldier", "tank", "armored_vehicle", "helicopter_gunship",
        "dogfight", "artillery", "mortar", "howitzer", "cannon", "machine_gun_fire", "rifle_fire",
        "æˆ˜äº‰", "æˆ˜å½¹", "æˆ˜æ–—", "å†›äº‹", "å£«å…µ", "å¦å…‹", "è£…ç”²è½¦", "æ­¦è£…ç›´å‡æœº",
        "ç©ºæˆ˜", "å¤§ç‚®", "è¿«å‡»ç‚®", "æ¦´å¼¹ç‚®", "åŠ å†œç‚®", "æœºæªæ‰«å°„", "æ­¥æªå°„å‡»"
    ],
    
    # ã€äººå£°ã€‘
    "äººå£°": [
        # è¯­è¨€
        "speech", "voice", "talk", "dialog", "conversation", "narration", "commentary", "announcement",
        "whisper", "shout", "yell", "scream", "laugh", "cry", "sob", "giggle", "chuckle",
        "å¯¹è¯", "è¯­éŸ³", "äº¤è°ˆ", "å¯¹è¯", "è°ˆè¯", "å™è¿°", "è§£è¯´", "å…¬å‘Š",
        "è€³è¯­", "å–Šå«", "å«å–Š", "å°–å«", "ç¬‘", "å“­", "æŠ½æ³£", "å’¯å’¯ç¬‘", "è½»ç¬‘",
        
        # éè¯­è¨€
        "breathing", "heavy_breathing", "panting", "gasping", "sigh", "yawn", "cough", "sneeze",
        "footsteps", "footstep", "walking", "running", "jumping", "landing", "clapping", "applause",
        "clothing", "rustle", "zipper", "button", "creak", "squeak", "thud", "thump",
        "å‘¼å¸", "æ€¥ä¿ƒå‘¼å¸", "å–˜æ¯", "å€’å¸æ°”", "å¹æ°”", "æ‰“å“ˆæ¬ ", "å’³å—½", "æ‰“å–·åš",
        "è„šæ­¥å£°", "æ­¥è¡Œ", "è·‘æ­¥", "è·³è·ƒ", "è½åœ°", "æ‹æ‰‹", "æŒå£°",
        "è¡£æœ", "æ‘©æ“¦å£°", "æ‹‰é“¾", "çº½æ‰£", "å±å‘€å£°", "å°–å«å£°", "é‡å‡»å£°", "ç °ç °å£°"
    ],
    
    # ã€å†·å…µå™¨ã€‘
    "å†·å…µå™¨": [
        # å…µå™¨ç±»å‹
        "sword", "blade", "knife", "dagger", "katana", "saber", "cutlass", "machete", "axe", "hammer",
        "mace", "flail", "spear", "lance", "bow", "arrow", "crossbow", "staff", "nunchaku", "chain",
        "åˆ€", "å‰‘", "åˆ€å‰‘", "åŒ•é¦–", "æ­¦å£«åˆ€", "å†›åˆ€", "æ°´æ‰‹åˆ€", "ç åˆ€", "æ–§", "é”¤",
        "ç‹¼ç‰™æ£’", "è¿æ·", "çŸ›", "é•¿çŸ›", "å¼“", "ç®­", "å¼©", "æ£", "åŒèŠ‚æ£", "é“¾",
        
        # åŠ¨ä½œä¸æ•ˆæœ
        "clash", "impact", "strike", "hit", "slash", "stab", "thrust", "parry", "block", "deflect",
        "whoosh", "swish", "slice", "cut", "chop", "smash", "crunch", "shatter", "break",
        "ç¢°æ’", "å†²å‡»", "æ‰“å‡»", "å‡»ä¸­", "æŒ¥ç ", "åˆºå‡»", "çªåˆº", "æ ¼æŒ¡", "é˜»æŒ¡", "åè½¬",
        "å—–å£°", "å˜¶å˜¶å£°", "åˆ‡å¼€", "åˆ‡å‰²", "åŠˆç ", "ç²‰ç¢", "å‹ç¢", "ç ´ç¢", "æ–­è£‚"
    ],
    
    # ã€UIäº¤äº’ã€‘
    "UIäº¤äº’": [
        # åŸºç¡€äº¤äº’
        "click", "button", "press", "select", "scroll", "drag", "drop", "hover", "menu", "navigation",
        "tap", "touch", "swipe", "pinch", "zoom", "rotate", "gesture", "flick", "slide",
        "ç‚¹å‡»", "æŒ‰é’®", "æŒ‰ä¸‹", "é€‰æ‹©", "æ»šåŠ¨", "æ‹–åŠ¨", "æ”¾ä¸‹", "æ‚¬åœ", "èœå•", "å¯¼èˆª",
        "è½»è§¦", "è§¦æ‘¸", "æ»‘åŠ¨", "æåˆ", "ç¼©æ”¾", "æ—‹è½¬", "æ‰‹åŠ¿", "è½»å¼¹", "æ»‘åŠ¨",
        
        # ç³»ç»Ÿåé¦ˆ
        "beep", "notification", "alert", "error", "warning", "success", "confirm", "cancel", "ding",
        "chime", "ping", "pop", "whoop", "system", "interface", "digital", "electronic", "sonar",
        "å“”å“”å£°", "é€šçŸ¥", "è­¦æŠ¥", "é”™è¯¯", "è­¦å‘Š", "æˆåŠŸ", "ç¡®è®¤", "å–æ¶ˆ", "å®å£°",
        "é’Ÿå£°", "å®å½“å£°", "å¼¹å‡ºå£°", "æ¬¢å‘¼å£°", "ç³»ç»Ÿ", "ç•Œé¢", "æ•°å­—", "ç”µå­", "å£°çº³"
    ],
    
    # ã€è½¬åœºéŸ³æ•ˆã€‘
    "è½¬åœºéŸ³æ•ˆ": [
        # ä¸Šå‡/ä¸‹é™
        "riser", "build", "tension_build", "upward", "climax", "ascend", "rise", "sweep_up", "swell",
        "downer", "fall", "release", "drop", "decay", "resolve", "descend", "fade_out", "collapse",
        "ä¸Šå‡", "æ„å»º", "ç´§å¼ æ„å»º", "å‘ä¸Š", "é«˜æ½®", "ä¸Šå‡", "å‡èµ·", "å‘ä¸Šæ‰«é¢‘", "è†¨èƒ€",
        "ä¸‹é™", "å è½", "é‡Šæ”¾", "æ‰è½", "è¡°å‡", "è§£å†³", "ä¸‹é™", "æ·¡å‡º", "å´©æºƒ",
        
        # è½¬åœºæ•ˆæœ
        "transition", "whoosh", "sweep", "fly_by", "pass", "slide", "zip", "swish", "crossfade",
        "stinger", "stab", "hit_short", "punctuation", "snap", "pop", "click", "switch", "change",
        "è½¬åœº", "å—–å£°", "æ‰«é¢‘", "é£è¿‡", "ç»è¿‡", "æ»‘åŠ¨", "æ‹‰é“¾", "å˜¶å˜¶å£°", "äº¤å‰æ·¡å…¥",
        "å¼ºè°ƒéŸ³", "åˆºè€³å£°", "çŸ­ä¿ƒå‘½ä¸­", "æ ‡ç‚¹", "å¼¹å“", "çˆ†ç ´å£°", "ç‚¹å‡»", "åˆ‡æ¢", "æ”¹å˜"
    ],
    
    # ã€ç”µå½±æ°›å›´ã€‘
    "ç”µå½±æ°›å›´": [
        # æƒ…ç»ªé“ºå«
        "atmosphere", "ambience", "mood", "tension", "suspense", "drama", "romance", "horror", "fear",
        "sadness", "joy", "peace", "calm", "serenity", "loneliness", "isolation", "despair", "hope",
        "æ°›å›´", "ç¯å¢ƒå£°", "æƒ…ç»ª", "ç´§å¼ ", "æ‚¬å¿µ", "æˆå‰§", "æµªæ¼«", "ææ€–", "ææƒ§",
        "æ‚²ä¼¤", "å–œæ‚¦", "å’Œå¹³", "å¹³é™", "å®é™", "å­¤ç‹¬", "å­¤ç«‹", "ç»æœ›", "å¸Œæœ›",
        
        # ç©ºé—´æ°›å›´
        "spatial", "surround", "dolby", "atmos", "5.1", "7.1", "reverb", "echo", "delay", "depth",
        "distance", "proximity", "perspective", "panning", "movement", "direction", "position", "location",
        "ç©ºé—´", "ç¯ç»•", "æœæ¯”", "å…¨æ™¯å£°", "5.1å£°é“", "7.1å£°é“", "æ··å“", "å›å£°", "å»¶è¿Ÿ", "æ·±åº¦",
        "è·ç¦»", "æ¥è¿‘", "é€è§†", "å£°åƒ", "ç§»åŠ¨", "æ–¹å‘", "ä½ç½®", "å®šä½"
    ],
    
    # ã€ä¸“ä¸šéŸ³æ•ˆç±»å‹ã€‘ï¼ˆå¢å¼ºç‰ˆï¼‰
    "ä¸“ä¸šç±»å‹": [
        # åŸºç¡€å£°å­¦
        "impact", "hit", "strike", "crash", "bang", "thud", "smash", "crunch", "clash", "collide",
        "riser", "build", "upward", "climax", "sweep", "whoosh", "fly_by", "pass", "transition",
        "pulse", "beat", "throb", "heartbeat", "rhythm", "cycle", "stinger", "stab", "accent",
        "drone", "bed", "pad", "atmosphere", "sustained", "background", "texture", "grit", "noise",
        
        # é«˜çº§æ•ˆæœ
        "glitch", "error", "digital_error", "stutter", "bit_crush", "reverse", "rewind", "backwards",
        "inverse", "flip", "morph", "transform", "shift", "evolve", "change", "granular", "particle",
        "cloud", "diffuse", "sci_fi", "alien", "laser", "energy", "futuristic", "synthetic",
        
        # ä¸­æ–‡ä¸“ä¸šæœ¯è¯­
        "å†²å‡»", "å‘½ä¸­", "æ‰“å‡»", "ç¢°æ’", "ç °", "é‡å‡»", "ç²‰ç¢", "å‹ç¢", "æ’å‡»", "ç›¸æ’",
        "ä¸Šå‡", "æ„å»º", "å‘ä¸Š", "é«˜æ½®", "æ‰«é¢‘", "å—–å£°", "é£è¿‡", "ç»è¿‡", "è½¬åœº",
        "è„‰å†²", "èŠ‚æ‹", "å¿ƒè·³", "å¿ƒè·³å£°", "èŠ‚å¥", "å¾ªç¯", "å¼ºè°ƒéŸ³", "åˆºè€³å£°", "é‡éŸ³",
        "æ°›å›´", "é“ºåº•", "å«åº•", "ç¯å¢ƒæ°›å›´", "æŒç»­", "èƒŒæ™¯", "çº¹ç†", "é¢—ç²’æ„Ÿ", "å™ªéŸ³",
        "æ•…éšœ", "é”™è¯¯", "æ•°å­—é”™è¯¯", "å¡é¡¿", "æ¯”ç‰¹å‹ç¼©", "åè½¬", "å€’å¸¦", "å‘å",
        "åå‘", "ç¿»è½¬", "å˜å½¢", "è½¬æ¢", "è½¬å˜", "è¿›åŒ–", "å˜åŒ–", "é¢—ç²’", "ç²’å­",
        "äº‘", "æ‰©æ•£", "ç§‘å¹»", "å¤–æ˜Ÿ", "æ¿€å…‰", "èƒ½é‡", "æœªæ¥", "åˆæˆ"
    ],
    
    # ã€åŠ¨ç‰©å£°éŸ³ã€‘
    "åŠ¨ç‰©å£°éŸ³": [
        # é‡ç”ŸåŠ¨ç‰©
        "dog", "bark", "howl", "growl", "whine", "cat", "meow", "purr", "hiss", "bird", "chirp",
        "songbird", "eagle", "hawk", "owl", "crow", "raven", "wolf", "howl", "lion", "roar",
        "tiger", "growl", "elephant", "trumpet", "monkey", "chimpanzee", "gorilla", "bear", "roar",
        "dog", "å å«", "åšå«", "å’†å“®", "å‘œå’½", "çŒ«", "å–µ", "å‘¼å™œ", "å˜¶å˜¶", "é¸Ÿ", "å•å•¾",
        "é¸£ç¦½", "é¹°", "éš¼", "çŒ«å¤´é¹°", "ä¹Œé¸¦", "æ¸¡é¸¦", "ç‹¼", "åšå«", "ç‹®å­", "å¼å«",
        "è€è™", "ä½å¼", "å¤§è±¡", "å–‡å­å£°", "çŒ´å­", "é»‘çŒ©çŒ©", "å¤§çŒ©çŒ©", "ç†Š", "å¼å«",
        
        # æ˜†è™«ä¸å°ç”Ÿç‰©
        "insect", "cricket", "chirp", "cicada", "bee", "buzz", "fly", "mosquito", "frog", "croak",
        "toad", "snake", "hiss", "rat", "squeak", "mouse", "bat", "echolocation", "whale", "song",
        "dolphin", "click", "seagull", "squawk", "chicken", "cluck", "rooster", "crow",
        "æ˜†è™«", "èŸ‹èŸ€", "å•å•¾", "è‰", "èœœèœ‚", "å—¡å—¡å£°", "è‹è‡", "èšŠå­", "é’è›™", "å‘±å‘±å«",
        "èŸ¾èœ", "è›‡", "å˜¶å˜¶å£°", "è€é¼ ", "å±å±å£°", "è™è ", "å£°çº³å®šä½", "é²¸é±¼", "æ­Œå£°",
        "æµ·è±š", "å’”å—’å£°", "æµ·é¸¥", "å°–å«å£°", "é¸¡", "å’¯å’¯å£°", "å…¬é¸¡", "æ‰“é¸£"
    ]
}

# ã€å…³é”®è¯æƒé‡è¡¨ã€‘- ä¸åŒæ¥æºçš„æƒé‡åˆ†é…
KEYWORD_WEIGHTS = {
    "filename_explicit": 0.9,    # æ–‡ä»¶åä¸­æ˜ç¡®çš„ä¸“ä¸šæœ¯è¯­
    "filename_context": 0.7,     # æ–‡ä»¶åä¸­çš„ä¸Šä¸‹æ–‡å…³é”®è¯
    "ai_high_confidence": 0.8,   # AIé«˜ç½®ä¿¡åº¦åˆ†æç»“æœ
    "ai_medium_confidence": 0.6, # AIä¸­ç­‰ç½®ä¿¡åº¦åˆ†æç»“æœ
    "ai_low_confidence": 0.4     # AIä½ç½®ä¿¡åº¦åˆ†æç»“æœ
}


# --- æ ¸å¿ƒå‡½æ•°ï¼šä¸­æ–‡éªŒè¯ä¸æ¸…æ´— ---
def is_valid_chinese(text):
    """ä¸¥æ ¼éªŒè¯æ˜¯å¦ä¸ºæœ‰æ•ˆä¸­æ–‡ï¼ˆåªå…è®¸ä¸­æ–‡ã€æ•°å­—ã€åŸºæœ¬æ ‡ç‚¹ï¼‰"""
    if not text or not isinstance(text, str):
        return False
    
    # å…è®¸çš„å­—ç¬¦èŒƒå›´ï¼š
    # \u4e00-\u9fffï¼šä¸­æ–‡å­—ç¬¦
    # \u3000-\u303fï¼šä¸­æ–‡æ ‡ç‚¹
    # \uff00-\uffefï¼šå…¨è§’å­—ç¬¦
    # 0-9ï¼šæ•°å­—
    # ç©ºæ ¼
    valid_chars = re.compile(r'^[\u4e00-\u9fff\u3000-\u303f\uff00-\uffef0-9\sã€ï¼Œã€‚ï¼›ï¼š\'"ã€ã€‘ã€Šã€‹ï¼ˆï¼‰()\-_]*$')
    
    # æ£€æŸ¥æ˜¯å¦åŒ…å«ä»»ä½•è‹±æ–‡å­—ç¬¦
    if re.search(r'[a-zA-Z]', text):
        return False
    
    # æ£€æŸ¥æ•´ä½“æ ¼å¼
    return bool(valid_chars.match(text.strip())) and len(text.strip()) > 0

def clean_chinese_text(text):
    """å½»åº•æ¸…æ´—æ–‡æœ¬ï¼Œåªä¿ç•™æœ‰æ•ˆä¸­æ–‡å­—ç¬¦"""
    if not text:
        return ""
    
    # 1. ç§»é™¤æ‰€æœ‰è‹±æ–‡å­—ç¬¦
    text = re.sub(r'[a-zA-Z]', '', text)
    
    # 2. ç§»é™¤ç‰¹æ®Šç¬¦å·ï¼ˆä¿ç•™ä¸­æ–‡æ ‡ç‚¹ï¼‰
    text = re.sub(r'[^\u4e00-\u9fff\u3000-\u303f\uff00-\uffef0-9\sã€ï¼Œã€‚ï¼›ï¼š\'"ã€ã€‘ã€Šã€‹ï¼ˆï¼‰()\-_]', '', text)
    
    # 3. ç§»é™¤å¤šä½™ç©ºæ ¼
    text = re.sub(r'\s+', ' ', text).strip()
    
    # 4. ç§»é™¤è¿ç»­é‡å¤å­—ç¬¦
    text = re.sub(r'(.)\1{2,}', r'\1\1', text)
    
    return text

def collect_audio_files(source_dirs, audio_exts):
    """
    ä»å¤šä¸ªæºç›®å½•ä¸­é€’å½’æ”¶é›†éŸ³é¢‘æ–‡ä»¶ã€‚

    :param source_dirs: æºç›®å½•åˆ—è¡¨ï¼ˆstr åˆ—è¡¨ï¼‰
    :param audio_exts: éŸ³é¢‘æ‰©å±•åå…ƒç»„ï¼Œå¦‚ ('.wav', '.mp3', '.flac')
    :return: å»é‡åçš„ç»å¯¹è·¯å¾„åˆ—è¡¨
    """
    if not isinstance(source_dirs, (list, tuple)):
        raise TypeError("source_dirs å¿…é¡»æ˜¯åˆ—è¡¨æˆ–å…ƒç»„")

    valid_dirs = []
    for d in source_dirs:
        if not isinstance(d, str):
            print(f"âš ï¸ è­¦å‘Šï¼šè·³è¿‡éå­—ç¬¦ä¸²è·¯å¾„: {d}")
            continue
        if not os.path.isdir(d):
            print(f"âš ï¸ è­¦å‘Šï¼šè·¯å¾„ä¸å­˜åœ¨æˆ–ä¸æ˜¯ç›®å½•ï¼Œå·²è·³è¿‡: {d}")
            continue
        valid_dirs.append(os.path.abspath(d))  # è½¬ä¸ºç»å¯¹è·¯å¾„ï¼Œä¾¿äºåç»­å¤„ç†

    if not valid_dirs:
        print("âŒ é”™è¯¯ï¼šæ²¡æœ‰æœ‰æ•ˆçš„æºç›®å½•ã€‚")
        return []

    files_set = set()
    for source_dir in valid_dirs:
        for root, _, filenames in os.walk(source_dir):
            for f in filenames:
                if f.lower().endswith(audio_exts):
                    full_path = os.path.abspath(os.path.join(root, f))
                    files_set.add(full_path)

    return sorted(files_set)  # æ’åºä¾¿äºè°ƒè¯•å’Œæ—¥å¿—ä¸€è‡´æ€§

def sanitize_filename(filename):
    """å®‰å…¨æ–‡ä»¶åæ¸…æ´—ï¼ˆä¿ç•™ä¸­æ–‡ï¼‰"""
    filename = re.sub(r'[\\/*?:"<>|]', '', filename).strip()
    filename = filename.replace('_', ' ')
    return filename

def get_file_md5(file_path):
    """è®¡ç®—æ–‡ä»¶MD5ï¼ˆå®‰å…¨å¤„ç†å¤§æ–‡ä»¶ï¼‰"""
    import hashlib
    hash_md5 = hashlib.md5()
    try:
        with open(file_path, "rb") as f:
            for chunk in iter(lambda: f.read(4096), b""):
                hash_md5.update(chunk)
        return hash_md5.hexdigest()
    except Exception as e:
        print(f"MD5è®¡ç®—å¤±è´¥ {file_path}: {str(e)}")
        return f"error_{abs(hash(str(e)))}"

def preprocess_audio(file_path):
    """é¢„å¤„ç†éŸ³é¢‘æ–‡ä»¶ï¼ˆå¢å¼ºçŸ­éŸ³æ•ˆæ”¯æŒï¼‰"""
    try:
        import numpy as np
        import librosa
        import math
        from scipy import signal
        
        # 1. è¯»å–éŸ³é¢‘ï¼ˆå…¼å®¹å„ç§æ ¼å¼ï¼‰
        waveform, sample_rate = sf.read(file_path, dtype='float32')
        original_duration = len(waveform) / sample_rate
        print(f"  ğŸ“ åŸå§‹éŸ³é¢‘æ—¶é•¿: {original_duration:.2f}ç§’")
        
        # 2. ç¡®ä¿æ˜¯å•å£°é“
        if len(waveform.shape) > 1:
            print(f"  ğŸ§ æ£€æµ‹åˆ°ç«‹ä½“å£°ï¼Œè½¬æ¢ä¸ºå•å£°é“")
            waveform = waveform.mean(axis=1)
        
        # 3. å¤„ç†æçŸ­éŸ³é¢‘ï¼ˆ<0.3ç§’ï¼‰- æ™ºèƒ½å¾ªç¯å¡«å……
        MIN_AUDIO_DURATION = 0.3  # ç§’
        if original_duration < MIN_AUDIO_DURATION:
            print(f"  âš ï¸ éŸ³é¢‘è¿‡çŸ­ ({original_duration:.2f}s)ï¼Œåº”ç”¨æ™ºèƒ½å¾ªç¯å¡«å……")
            
            # è®¡ç®—ç›®æ ‡æ ·æœ¬æ•°
            target_samples = int(MIN_AUDIO_DURATION * sample_rate)
            
            # æ™ºèƒ½å¾ªç¯ï¼šé¿å…çªå…€çš„æ¥ç¼
            if len(waveform) > 0:
                # æ–¹æ³•1ï¼šæ·¡å…¥æ·¡å‡ºå¾ªç¯
                fade_samples = min(int(0.02 * sample_rate), len(waveform) // 4)
                fade_in = np.linspace(0, 1, fade_samples)
                fade_out = np.linspace(1, 0, fade_samples)
                
                # åº”ç”¨æ·¡å…¥æ·¡å‡º
                if len(waveform) > fade_samples * 2:
                    waveform[:fade_samples] *= fade_in
                    waveform[-fade_samples:] *= fade_out
                
                # å¾ªç¯å¡«å……
                repeats = math.ceil(target_samples / len(waveform))
                extended = np.tile(waveform, repeats)[:target_samples]
                
                # äº¤å‰æ·¡åŒ–æ¥ç¼
                for i in range(1, repeats):
                    start_idx = i * len(waveform) - fade_samples // 2
                    end_idx = i * len(waveform) + fade_samples // 2
                    if start_idx < 0 or end_idx >= len(extended):
                        continue
                    
                    # äº¤å‰æ·¡åŒ–
                    cross_fade = np.linspace(1, 0, fade_samples)
                    extended[start_idx:start_idx+fade_samples] *= cross_fade
                    extended[end_idx-fade_samples:end_idx] *= (1 - cross_fade)
                
                waveform = extended
            else:
                # ç©ºéŸ³é¢‘å¤„ç†
                waveform = np.zeros(target_samples)
        
        # 4. é‡é‡‡æ ·åˆ°16kHzï¼ˆASTæ¨¡å‹è¦æ±‚ï¼‰
        if sample_rate != 16000:
            print(f"  ğŸ”Š é‡é‡‡æ ·: {sample_rate}Hz â†’ 16000Hz")
            waveform = librosa.resample(waveform, orig_sr=sample_rate, target_sr=16000)
            sample_rate = 16000
        
        # 5. æ™ºèƒ½å¢å¼ºï¼ˆé’ˆå¯¹çŸ­éŸ³æ•ˆï¼‰
        SHORT_AUDIO_THRESHOLD = 1.5  # ç§’
        current_duration = len(waveform) / sample_rate
        
        if current_duration < SHORT_AUDIO_THRESHOLD:
            print(f"  ğŸ”§ æ£€æµ‹åˆ°çŸ­éŸ³æ•ˆ ({current_duration:.2f}s)ï¼Œåº”ç”¨ä¸“ä¸šå¢å¼º")
            
            # 5.1. æ™ºèƒ½æ—¶é—´æ‹‰ä¼¸ï¼ˆ0.3-1.5ç§’ï¼‰
            if 0.3 <= current_duration < SHORT_AUDIO_THRESHOLD:
                target_duration = min(SHORT_AUDIO_THRESHOLD, current_duration * 1.8)
                target_samples = int(target_duration * sample_rate)
                
                print(f"  â±ï¸ æ—¶é—´æ‹‰ä¼¸: {current_duration:.2f}s â†’ {target_duration:.2f}s")
                
                # ä½¿ç”¨ç›¸ä½å£°ç å™¨è¿›è¡Œé«˜è´¨é‡æ—¶é—´æ‹‰ä¼¸
                try:
                    # æ–¹æ³•ï¼šä½¿ç”¨librosaçš„time_stretchï¼ˆé«˜è´¨é‡ï¼‰
                    stretch_ratio = current_duration / target_duration
                    waveform = librosa.effects.time_stretch(waveform, rate=stretch_ratio)
                    waveform = waveform[:target_samples]  # ç¡®ä¿é•¿åº¦æ­£ç¡®
                    
                    # å¦‚æœæ‹‰ä¼¸åå¤ªçŸ­ï¼Œè¡¥é›¶
                    if len(waveform) < target_samples:
                        waveform = np.pad(waveform, (0, target_samples - len(waveform)), 'constant')
                    
                    current_duration = len(waveform) / sample_rate
                    print(f"  âœ… é«˜è´¨é‡æ—¶é—´æ‹‰ä¼¸å®Œæˆï¼Œæ–°æ—¶é•¿: {current_duration:.2f}s")
                    
                except Exception as e:
                    print(f"  âš ï¸ é«˜è´¨é‡æ‹‰ä¼¸å¤±è´¥ï¼Œä½¿ç”¨å¤‡ç”¨æ–¹æ³•: {str(e)}")
                    # å¤‡ç”¨æ–¹æ³•ï¼šçº¿æ€§æ’å€¼
                    waveform = np.interp(
                        np.linspace(0, len(waveform), target_samples),
                        np.arange(len(waveform)),
                        waveform
                    )
            
            # 5.2. é¢‘è°±å¢å¼ºï¼ˆæ‰€æœ‰çŸ­éŸ³æ•ˆï¼‰
            print("  ğŸ“Š åº”ç”¨é¢‘è°±å¢å¼º...")
            try:
                # é«˜é€šæ»¤æ³¢ï¼ˆç§»é™¤<80Hzçš„ä½é¢‘å™ªå£°ï¼‰
                nyquist = sample_rate / 2
                b, a = signal.butter(2, 80 / nyquist, btype='high')
                waveform = signal.filtfilt(b, a, waveform)
                
                # å‡è¡¡å™¨å¢å¼ºï¼ˆæå‡å…³é”®é¢‘æ®µï¼‰
                freq_ranges = [
                    (200, 500, 1.2),    # 20% å¢ç›Š - åŸºç¡€æŒ¯åŠ¨
                    (1000, 4000, 1.3),  # 30% å¢ç›Š - æè´¨ç»†èŠ‚
                    (5000, 8000, 1.15)  # 15% å¢ç›Š - ç©ºé—´æ„Ÿ
                ]
                
                # ä½¿ç”¨STFTè¿›è¡Œé¢‘æ®µå¢å¼º
                n_fft = 2048
                hop_length = 512
                stft = librosa.stft(waveform, n_fft=n_fft, hop_length=hop_length)
                freqs = librosa.fft_frequencies(sr=sample_rate, n_fft=n_fft)
                
                for low, high, gain in freq_ranges:
                    # æ‰¾åˆ°ç›®æ ‡é¢‘æ®µç´¢å¼•
                    idx = np.where((freqs >= low) & (freqs <= high))[0]
                    if len(idx) > 0:
                        stft[idx, :] *= gain
                
                # é€†STFT
                waveform = librosa.istft(stft, hop_length=hop_length, length=len(waveform))
                
                print("  âœ… é¢‘è°±å¢å¼ºå®Œæˆ")
            except Exception as e:
                print(f"  âš ï¸ é¢‘è°±å¢å¼ºå¤±è´¥: {str(e)}")
            
            # 5.3. åŠ¨æ€èŒƒå›´å‹ç¼©ï¼ˆæå‡å¼±ä¿¡å·ï¼‰
            try:
                rms = np.sqrt(np.mean(waveform**2))
                if rms > 0.001:  # é¿å…é™¤é›¶
                    # å‹ç¼©æ¯” 2:1ï¼Œé˜ˆå€¼ -20dB
                    threshold = 0.1  # -20dB
                    ratio = 2.0
                    
                    mask = np.abs(waveform) > threshold
                    waveform[mask] = threshold + (np.abs(waveform[mask]) - threshold) / ratio * np.sign(waveform[mask])
                    
                    # æå‡æ•´ä½“å¢ç›Š
                    waveform *= 1.2
                    
                    print("  ğŸ“ˆ åŠ¨æ€èŒƒå›´ä¼˜åŒ–å®Œæˆ")
            except Exception as e:
                print(f"  âš ï¸ åŠ¨æ€èŒƒå›´å¤„ç†å¤±è´¥: {str(e)}")
        
        # 6. å½’ä¸€åŒ–åˆ°[-1, 1]
        max_val = np.max(np.abs(waveform))
        if max_val > 0:
            waveform = waveform / (max_val + 1e-8)
            print(f"  âœ… å½’ä¸€åŒ–å®Œæˆï¼Œå³°å€¼: {max_val:.4f}")
        
        # 7. ç¡®ä¿æœ€å°é•¿åº¦ï¼ˆASTæ¨¡å‹è¦æ±‚ï¼‰
        MIN_SAMPLES = 1600  # 0.1ç§’ at 16kHz
        if len(waveform) < MIN_SAMPLES:
            print(f"  âš ï¸ éŸ³é¢‘ä»è¿‡çŸ­ ({len(waveform)} samples)ï¼Œå¡«å……åˆ° {MIN_SAMPLES} samples")
            waveform = np.pad(waveform, (0, MIN_SAMPLES - len(waveform)), 'constant')
        
        # 8. è½¬æ¢ä¸ºPyTorchå¼ é‡
        waveform_tensor = torch.tensor(waveform).unsqueeze(0)
        final_duration = waveform_tensor.shape[1] / sample_rate
        print(f"  ğŸ¯ é¢„å¤„ç†å®Œæˆï¼Œæœ€ç»ˆæ—¶é•¿: {final_duration:.2f}ç§’ï¼Œå½¢çŠ¶: {waveform_tensor.shape}")
        
        return waveform_tensor
        
    except ImportError as e:
        print(f"âš ï¸ ç¼ºå°‘ä¾èµ–åº“: {str(e)}")
        print("ğŸ’¡ è¯·å®‰è£…: pip install numpy librosa scipy")
        # è¿”å›3ç§’ç©ºç™½éŸ³é¢‘ï¼ˆå…¼å®¹æ¨¡å¼ï¼‰
        return torch.zeros(1, 16000 * 3)
    
    except Exception as e:
        print(f"âŒ éŸ³é¢‘é¢„å¤„ç†å¤±è´¥ {file_path}: {str(e)}")
        print(f"   é”™è¯¯ç±»å‹: {type(e).__name__}")
        print(f"   é”™è¯¯è¯¦æƒ…: {str(e)}")
        
        # æ™ºèƒ½é”™è¯¯æ¢å¤
        if "soundfile" in str(e).lower() or "format" in str(e).lower():
            print("  ğŸ”„ å°è¯•å¤‡ç”¨éŸ³é¢‘åŠ è½½æ–¹æ³•...")
            try:
                import wave
                with wave.open(file_path, 'rb') as wav_file:
                    n_channels = wav_file.getnchannels()
                    sample_width = wav_file.getsampwidth()
                    frame_rate = wav_file.getframerate()
                    n_frames = wav_file.getnframes()
                    
                    frames = wav_file.readframes(n_frames)
                    if sample_width == 2:  # 16-bit
                        waveform = np.frombuffer(frames, dtype=np.int16).astype(np.float32) / 32768.0
                    else:  # 8-bit or 24-bit
                        waveform = np.frombuffer(frames, dtype=np.uint8 if sample_width == 1 else np.int32).astype(np.float32)
                        waveform = waveform / (256 if sample_width == 1 else 2147483648.0)
                    
                    if n_channels > 1:
                        waveform = waveform.reshape(-1, n_channels).mean(axis=1)
                    
                    # é‡é‡‡æ ·åˆ°16kHz
                    if frame_rate != 16000:
                        import librosa
                        waveform = librosa.resample(waveform, orig_sr=frame_rate, target_sr=16000)
                    
                    return torch.tensor(waveform).unsqueeze(0)
            except Exception as fallback_e:
                print(f"  âŒ å¤‡ç”¨æ–¹æ³•ä¹Ÿå¤±è´¥: {str(fallback_e)}")
        
        # æœ€ç»ˆå›é€€ï¼š3ç§’ç©ºç™½éŸ³é¢‘
        print("  ğŸ›¡ï¸ ä½¿ç”¨å®‰å…¨å›é€€ï¼š3ç§’ç©ºç™½éŸ³é¢‘")
        return torch.zeros(1, 16000 * 3)

def generate_readable_filename(description, file_ext, file_hash, max_length=40):
    """ç”Ÿæˆç¾è§‚æ–‡ä»¶å"""
    clean_desc = re.sub(r'[\\/*?:"<>|]', '', description).strip()
    
    # ä»æè¿°ä¸­æå–æ ¸å¿ƒåœºæ™¯ï¼ˆå‰10ä¸ªä¸­æ–‡å­—ï¼‰
    core_scene = ''.join(re.findall(r'[\u4e00-\u9fa5]', clean_desc)[:10])
    
    # å¦‚æœæå–çš„åœºæ™¯å¤ªçŸ­ï¼Œä½¿ç”¨å‰5ä¸ªè¯
    if len(core_scene) < 4:
        words = re.findall(r'[\u4e00-\u9fa5]{1,4}', clean_desc)
        core_scene = ''.join(words[:3]) if words else "éŸ³æ•ˆç´ æ"
    
    short_hash = file_hash[:6]
    filename = f"{core_scene}-{short_hash}{file_ext}"
    
    if len(filename) > max_length:
        filename = filename[:max_length - len(file_ext) - 1] + file_ext
    
    if len(core_scene) < 2:
        filename = f"éŸ³æ•ˆç´ æ-{short_hash}{file_ext}"
    
    return filename

def extract_filename_keywords(original_filename):
    """
    ä»åŸå§‹æ–‡ä»¶åä¸­æå–ä¸“ä¸šå…³é”®è¯
    ç¤ºä¾‹: "01åŸå¸‚ç¯å…‰äº¤é€šå’Œæ­¥è¡ŒåŸå¸‚éš†éš† 01 City,Light Traffic And Pedestrians,City Rumble å¤©é€”å½±åƒ.wav"
    è¾“å‡º: {
        "explicit_keywords": ["äº¤é€š", "åŸå¸‚éš†éš†", "traffic", "city rumble"],
        "context_keywords": ["ç¯å…‰", "æ­¥è¡Œ", "light", "pedestrians"],
        "source_info": ["å¤©é€”å½±åƒ"]
    }
    """
    # 1. ç§»é™¤æ‰©å±•åå’Œåºå·
    filename = os.path.splitext(original_filename)[0]
    filename = re.sub(r'^\d+[_\-]?', '', filename).strip()
    
    # 2. ç§»é™¤æ¥æºä¿¡æ¯ï¼ˆå¦‚"å¤©é€”å½±åƒ"ï¼‰
    source_info = []
    source_match = re.search(r'[_\-\s]([^\s_\-]+å½±åƒ|sound[_\s]?library|audio[_\s]?archive|recording)[_\-\s]?$', filename, re.IGNORECASE)
    if source_match:
        source_info = [source_match.group(1).strip()]
        filename = filename[:source_match.start()].strip()
    
    # 3. åˆ†å‰²ä¸­è‹±æ–‡éƒ¨åˆ†
    chinese_parts = re.findall(r'[\u4e00-\u9fa5][^a-zA-Z]*', filename)
    english_parts = re.findall(r'[a-zA-Z][^\\u4e00-\\u9fa5]*', filename)
    
    # 4. æå–æ˜¾å¼å…³é”®è¯ï¼ˆé€šå¸¸åŒ…å«éŸ³æ•ˆç±»å‹ï¼‰
    explicit_keywords = []
    context_keywords = []
    
    # ä¸­æ–‡æ˜¾å¼å…³é”®è¯
    for part in chinese_parts:
        # ä¸“ä¸šéŸ³æ•ˆç±»å‹
        for keyword in PROFESSIONAL_KEYWORDS["ä¸“ä¸šç±»å‹"]:
            if keyword in part and len(keyword) >= 2:
                explicit_keywords.append(keyword)
                part = part.replace(keyword, '')
        
        # å…¶ä»–ä¸“ä¸šå…³é”®è¯
        words = [w for w in re.split(r'[^\u4e00-\u9fa5]', part) if len(w) >= 2]
        if words:
            # é•¿åº¦å¤§äº3çš„è¯æ›´å¯èƒ½æ˜¯ä¸“ä¸šæœ¯è¯­
            explicit_keywords.extend([w for w in words if len(w) >= 3])
            context_keywords.extend([w for w in words if len(w) == 2])
    
    # è‹±æ–‡æ˜¾å¼å…³é”®è¯
    for part in english_parts:
        part = part.lower().strip()
        # ä¸“ä¸šéŸ³æ•ˆç±»å‹
        for keyword in PROFESSIONAL_KEYWORDS["ä¸“ä¸šç±»å‹"]:
            if keyword.lower() in part:
                clean_keyword = re.sub(r'[^a-z\s]', '', keyword.lower()).strip()
                if clean_keyword and len(clean_keyword) >= 3:
                    explicit_keywords.append(clean_keyword)
        
        # å…¶ä»–ä¸“ä¸šå…³é”®è¯
        words = [w.strip() for w in re.split(r'[^a-z]', part) if len(w) >= 2]
        if words:
            explicit_keywords.extend([w for w in words if len(w) >= 4])
            context_keywords.extend([w for w in words if 2 <= len(w) < 4])
    
    # 5. å»é‡å’Œæ¸…æ´—
    explicit_keywords = list(dict.fromkeys([k.strip() for k in explicit_keywords if k.strip()]))
    context_keywords = list(dict.fromkeys([k.strip() for k in context_keywords if k.strip()]))
    source_info = list(dict.fromkeys([s.strip() for s in source_info if s.strip()]))
    
    return {
        "explicit_keywords": explicit_keywords[:5],  # æœ€å¤š5ä¸ªæ˜¾å¼å…³é”®è¯
        "context_keywords": context_keywords[:5],    # æœ€å¤š5ä¸ªä¸Šä¸‹æ–‡å…³é”®è¯
        "source_info": source_info
    }

def calculate_keyword_confidence(keywords, category):
    """
    è®¡ç®—å…³é”®è¯ä¸åˆ†ç±»çš„åŒ¹é…ç½®ä¿¡åº¦
    è¿”å›: (confidence_score, matched_keywords)
    """
    if not keywords or not category:
        return 0.0, []
    
    matched_keywords = []
    confidence_score = 0.0
    
    # 1. æ£€æŸ¥æ˜¾å¼å…³é”®è¯
    for keyword in keywords.get("explicit_keywords", []):
        # æ£€æŸ¥æ˜¯å¦åœ¨ä¸“ä¸šå…³é”®è¯åº“ä¸­
        for cat, kw_list in PROFESSIONAL_KEYWORDS.items():
            if cat == category and any(kw.lower() in keyword.lower() for kw in kw_list):
                confidence_score += KEYWORD_WEIGHTS["filename_explicit"]
                matched_keywords.append(keyword)
                break
    
    # 2. æ£€æŸ¥ä¸Šä¸‹æ–‡å…³é”®è¯
    for keyword in keywords.get("context_keywords", []):
        for cat, kw_list in PROFESSIONAL_KEYWORDS.items():
            if cat == category and any(kw.lower() in keyword.lower() for kw in kw_list):
                confidence_score += KEYWORD_WEIGHTS["filename_context"] * 0.8  # ç•¥ä½æƒé‡
                matched_keywords.append(keyword)
                break
    
    # 3. é˜²æ­¢è¿‡åº¦è‡ªä¿¡
    max_possible = len(keywords.get("explicit_keywords", [])) * KEYWORD_WEIGHTS["filename_explicit"] + \
                  len(keywords.get("context_keywords", [])) * KEYWORD_WEIGHTS["filename_context"]
    
    if max_possible > 0:
        confidence_score = min(confidence_score / max_possible, 0.95)
    
    return confidence_score, matched_keywords

class AIEngine:
    def __init__(self, model_name, cache_dir, is_offline, ollama_api, qwen_model):
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        print(f"ğŸš€ GPUåŠ é€ŸçŠ¶æ€: {self.device}") 
        self.ollama_api = ollama_api
        self.qwen_model = qwen_model
        
        # åˆ›å»ºç¼“å­˜ç›®å½•
        os.makedirs(cache_dir, exist_ok=True)
        
        # åˆå§‹åŒ–ASTæ¨¡å‹
        try:
            from transformers import AutoFeatureExtractor, AutoModelForAudioClassification
            self.feature_extractor = AutoFeatureExtractor.from_pretrained(
                model_name, cache_dir=cache_dir, local_files_only=is_offline
            )
            self.audio_model = AutoModelForAudioClassification.from_pretrained(
                model_name, cache_dir=cache_dir, local_files_only=is_offline
            ).to(self.device)
            print(f"âœ… æˆåŠŸåŠ è½½ASTæ¨¡å‹: {model_name}")
        except Exception as e:
            print(f"âš ï¸ ASTæ¨¡å‹åŠ è½½å¤±è´¥: {str(e)}")
            self.audio_model = None
            self.feature_extractor = None
            
        # æ–°å¢ï¼šçŸ­éŸ³æ•ˆä¸“ç”¨é…ç½®
        self.SHORT_AUDIO_THRESHOLD = 1.5  # ç§’ï¼ˆçŸ­äº1.5ç§’è§†ä¸ºçŸ­éŸ³æ•ˆï¼‰
        self.MIN_AUDIO_DURATION = 0.3     # ç§’ï¼ˆæœ€çŸ­æœ‰æ•ˆéŸ³é¢‘ï¼‰
        
        # çŸ­éŸ³æ•ˆä¸“ç”¨å…³é”®è¯åº“
        self.SHORT_SFX_KEYWORDS = {
            "impact_short": ["click", "tap", "snap", "pop", "thud_short", "hit_short", "stinger", "stab", 
                            "ç‚¹å‡»", "è½»è§¦", "å¼¹å“", "çˆ†ç ´", "çŸ­ä¿ƒ", "å¼ºè°ƒ", "ç¬é—´"],
            "material_short": ["rustle", "crinkle", "crunch_short", "paper", "cloth", "plastic", "foil", 
                              "æ‘©æ“¦", "æ‰æ“", "çº¸", "å¸ƒæ–™", "å¡‘æ–™", "é”¡çº¸", "è–„è†œ"],
            "nature_short": ["drop", "drip", "splash_small", "leaf_rustle", "bird_chirp_short", 
                            "æ°´æ»´", "æ»´è½", "å°æ°´èŠ±", "æ ‘å¶", "é¸Ÿå«çŸ­ä¿ƒ"],
            "ui_short": ["beep_short", "blip", "bloop", "ding_short", "error_short", "success_short", 
                        "çŸ­å“”å£°", "æ»´ç­”", "æç¤ºéŸ³", "é”™è¯¯çŸ­éŸ³", "æˆåŠŸçŸ­éŸ³"]
        }

    def _enhance_short_audio(self, waveform, sample_rate=16000):
        """
        ä¸“ä¸šå¢å¼ºçŸ­éŸ³æ•ˆï¼ˆ<1.5ç§’ï¼‰
        1. æ™ºèƒ½æ—¶é—´æ‹‰ä¼¸ï¼ˆä¿æŒéŸ³é«˜ï¼‰
        2. é¢‘è°±å¢å¼º
        3. å¾ªç¯å¡«å……ï¼ˆå¯é€‰ï¼‰
        """
        duration = waveform.shape[1] / sample_rate
        
        # 1. å¤„ç†æçŸ­éŸ³é¢‘ï¼ˆ<0.3ç§’ï¼‰
        if duration < self.MIN_AUDIO_DURATION:
            print(f"  âš ï¸ éŸ³é¢‘è¿‡çŸ­ ({duration:.2f}s)ï¼Œä½¿ç”¨å¾ªç¯å¡«å……")
            target_samples = int(self.MIN_AUDIO_DURATION * sample_rate)
            repeats = math.ceil(target_samples / waveform.shape[1])
            waveform = torch.tile(waveform, (1, repeats))[:, :target_samples]
            return waveform
        
        # 2. æ™ºèƒ½æ—¶é—´æ‹‰ä¼¸ï¼ˆ0.3-1.5ç§’ï¼‰
        if duration < self.SHORT_AUDIO_THRESHOLD:
            target_duration = min(1.5, duration * 1.8)  # æœ€å¤šæ‹‰ä¼¸åˆ°1.5ç§’
            target_samples = int(target_duration * sample_rate)
            
            print(f"  ğŸ”§ å¢å¼ºçŸ­éŸ³æ•ˆ: {duration:.2f}s â†’ {target_duration:.2f}s")
            
            # ä½¿ç”¨ç›¸ä½å£°ç å™¨è¿›è¡Œé«˜è´¨é‡æ—¶é—´æ‹‰ä¼¸ï¼ˆä¿æŒéŸ³é«˜ï¼‰
            try:
                import numpy as np
                from scipy.signal import resample
                
                # è½¬æ¢ä¸ºnumpyè¿›è¡Œå¤„ç†
                audio_np = waveform.squeeze().numpy()
                
                # é«˜è´¨é‡é‡é‡‡æ ·ï¼ˆä¿æŒéŸ³é«˜ï¼‰
                stretched_audio = resample(audio_np, target_samples)
                
                # è½¬å›PyTorchå¼ é‡
                waveform = torch.tensor(stretched_audio).unsqueeze(0)
            except Exception as e:
                print(f"  âš ï¸ æ—¶é—´æ‹‰ä¼¸å¤±è´¥ï¼Œä½¿ç”¨ç®€å•é‡é‡‡æ ·: {str(e)}")
                # å¤‡ç”¨æ–¹æ¡ˆï¼šç®€å•é‡é‡‡æ ·
                waveform = torch.nn.functional.interpolate(
                    waveform.unsqueeze(0), 
                    size=target_samples, 
                    mode='linear',
                    align_corners=False
                ).squeeze(0)
        
        # 3. é¢‘è°±å¢å¼ºï¼ˆæ‰€æœ‰çŸ­éŸ³æ•ˆï¼‰
        if duration < self.SHORT_AUDIO_THRESHOLD:
            try:
                # åº”ç”¨è½»å¾®çš„å‡è¡¡å™¨å¢å¼ºï¼ˆé‡ç‚¹æå‡200Hz-8kHzèŒƒå›´ï¼‰
                waveform = self._apply_spectral_enhancement(waveform, sample_rate)
            except Exception as e:
                print(f"  âš ï¸ é¢‘è°±å¢å¼ºå¤±è´¥: {str(e)}")
        
        return waveform

    def _apply_spectral_enhancement(self, waveform, sample_rate=16000):
        """é¢‘è°±å¢å¼ºï¼šæå‡çŸ­éŸ³æ•ˆçš„ç‰¹å¾æ¸…æ™°åº¦"""
        import numpy as np
        from scipy import signal
        
        audio_np = waveform.squeeze().numpy()
        
        # 1. åº”ç”¨é«˜é€šæ»¤æ³¢å™¨ï¼ˆç§»é™¤<80Hzçš„ä½é¢‘å™ªå£°ï¼‰
        b, a = signal.butter(2, 80/(sample_rate/2), btype='high')
        audio_np = signal.filtfilt(b, a, audio_np)
        
        # 2. åº”ç”¨å‡è¡¡å™¨å¢å¼ºï¼ˆæå‡å…³é”®é¢‘æ®µï¼‰
        # 200-500Hz: äººå£°/ç‰©ä½“åŸºç¡€
        # 1-4kHz: æè´¨ç»†èŠ‚
        # 5-8kHz: ç©ºé—´æ„Ÿ/ç©ºæ°”æ„Ÿ
        freq_ranges = [
            (200, 500, 1.2),    # 20% å¢ç›Š
            (1000, 4000, 1.3),  # 30% å¢ç›Š
            (5000, 8000, 1.15)  # 15% å¢ç›Š
        ]
        
        for low, high, gain in freq_ranges:
            # ä½¿ç”¨FFTè¿›è¡Œé¢‘æ®µå¢å¼º
            spectrum = np.fft.rfft(audio_np)
            freqs = np.fft.rfftfreq(len(audio_np), 1/sample_rate)
            
            # æ‰¾åˆ°ç›®æ ‡é¢‘æ®µç´¢å¼•
            idx = np.where((freqs >= low) & (freqs <= high))[0]
            if len(idx) > 0:
                spectrum[idx] *= gain
        
            # é€†å˜æ¢å›æ—¶åŸŸ
            audio_np = np.fft.irfft(spectrum, n=len(audio_np))
        
        # 3. åŠ¨æ€èŒƒå›´å‹ç¼©ï¼ˆæå‡å¼±ä¿¡å·ï¼‰
        rms = np.sqrt(np.mean(audio_np**2))
        if rms > 0:
            # å¯¹ä½äºRMSçš„ä¿¡å·è¿›è¡Œæå‡
            mask = np.abs(audio_np) < rms
            audio_np[mask] *= 1.5  # 50% å¢ç›Š
        
        # 4. é™åˆ¶å³°å€¼é˜²æ­¢å¤±çœŸ
        max_val = np.max(np.abs(audio_np))
        if max_val > 0.99:
            audio_np = audio_np * 0.99 / max_val
        
        return torch.tensor(audio_np).unsqueeze(0)

    def _calculate_spectral_centroid(self, waveform, sample_rate=16000):
        """è®¡ç®—é¢‘è°±è´¨å¿ƒ"""
        try:
            if waveform.shape[1] < 1024:
                return 0
            
            segment = waveform[0, :1024].numpy()
            spectrum = np.abs(np.fft.rfft(segment))
            freqs = np.fft.rfftfreq(len(segment), 1/sample_rate)
            spectral_centroid = np.sum(spectrum * freqs) / np.sum(spectrum)
            return float(spectral_centroid)
        except:
            return 0

    def _generate_acoustic_fingerprint(self, waveform, sample_rate=16000):
        """ç”Ÿæˆå£°å­¦æŒ‡çº¹"""
        try:
            duration = waveform.shape[1] / sample_rate
            rms = torch.sqrt(torch.mean(waveform**2)).item()
            spectral_centroid = self._calculate_spectral_centroid(waveform, sample_rate)
            
            fingerprint = (
                f"æŒç»­æ—¶é—´:{duration:.1f}s, "
                f"åŠ¨æ€èŒƒå›´:{rms*100:.1f}%, "
                f"ä¸»å¯¼é¢‘ç‡:{spectral_centroid:.0f}Hz"
            )
            return fingerprint
        except Exception as e:
            print(f"âš ï¸ å£°å­¦æŒ‡çº¹ç”Ÿæˆå¤±è´¥: {str(e)}")
            return "ä¸“ä¸šéŸ³æ•ˆç‰¹å¾, é€šç”¨é¢‘è°±åˆ†æ"

    def classify_audio(self, waveform, categories, original_filename):
        """èåˆæ–‡ä»¶ååˆ†æçš„ç²¾å‡†åˆ†ç±»ï¼ˆå¢å¼ºçŸ­éŸ³æ•ˆæ”¯æŒï¼‰"""
        if self.audio_model is None:
            return "æœªåˆ†ç±»ç´ æ", 0.0
        
        try:
            # 1. æ£€æµ‹éŸ³é¢‘æ—¶é•¿
            duration = waveform.shape[1] / 16000  # sample_rate=16000
            
            # 2. çŸ­éŸ³æ•ˆä¸“é¡¹å¤„ç†
            is_short_audio = duration < self.SHORT_AUDIO_THRESHOLD
            original_waveform = waveform.clone() if is_short_audio else None
            
            if is_short_audio:
                print(f"â±ï¸ æ£€æµ‹åˆ°çŸ­éŸ³æ•ˆ: {duration:.2f}ç§’")
                waveform = self._enhance_short_audio(waveform)
            
            # 3. ASTæ¨¡å‹åˆ†æ
            inputs = self.feature_extractor(
                waveform.squeeze().numpy(), 
                sampling_rate=16000, 
                return_tensors="pt"
            ).to(self.device)
            
            with torch.no_grad():
                outputs = self.audio_model(**inputs)
                probs = torch.nn.functional.softmax(outputs.logits, dim=-1)
                _, top_idxs = probs[0].topk(3)
                confidence_scores = probs[0][top_idxs].cpu().numpy()
            
            ast_labels = [self.audio_model.config.id2label[idx.item()] for idx in top_idxs]
            ast_category = self._map_to_simple_category(ast_labels, categories)
            ast_confidence = float(confidence_scores[0])
            
            # 4. æ–‡ä»¶åå…³é”®è¯åˆ†æ
            filename_keywords = extract_filename_keywords(original_filename)
            filename_category, filename_confidence = self._analyze_filename_for_category(
                filename_keywords, categories
            )
            
            # 5. èåˆå†³ç­–
            final_category, final_confidence = self._fuse_decisions_short_audio(
                ast_category, ast_confidence,
                filename_category, filename_confidence,
                is_short_audio, duration,
                filename_keywords, ast_labels
            )
            
            # 6. ä½ç½®ä¿¡åº¦ä¿®å¤
            final_category, final_confidence = self._repair_low_confidence(
                waveform, final_category, final_confidence, filename_keywords
            )
            
            print(f"ğŸ¯ çŸ­éŸ³æ•ˆèåˆ: åŸå§‹({ast_category}, {ast_confidence:.2f}) + æ–‡ä»¶å({filename_category}, {filename_confidence:.2f}) â†’ {final_category}, {final_confidence:.2f}")
            
            return final_category, final_confidence
            
        except Exception as e:
            print(f"åˆ†ç±»å¤±è´¥: {str(e)}")
            return "æœªåˆ†ç±»ç´ æ", 0.0

    def _fuse_decisions_short_audio(self, ast_category, ast_confidence, 
                                   filename_category, filename_confidence,
                                   is_short_audio, duration,
                                   filename_keywords, ast_labels):
        """
        çŸ­éŸ³æ•ˆä¸“ç”¨å†³ç­–èåˆ
        åŸåˆ™ï¼š
        1. çŸ­éŸ³æ•ˆä¼˜å…ˆä¿¡ä»»æ–‡ä»¶å
        2. ä½ç½®ä¿¡åº¦æ—¶ä½¿ç”¨ä¿å®ˆç­–ç•¥
        3. è¯†åˆ«ç‰¹æ®ŠçŸ­éŸ³æ•ˆç±»å‹ï¼ˆimpact/æè´¨/è‡ªç„¶ï¼‰
        """
        # 1. é¦–å…ˆæ£€æŸ¥æ˜¯å¦ä¸ºç‰¹æ®ŠçŸ­éŸ³æ•ˆç±»å‹
        short_sfx_category = self._detect_short_sfx_type(filename_keywords, ast_labels, duration)
        if short_sfx_category and short_sfx_category in ["è½¬åœºéŸ³æ•ˆ", "UIäº¤äº’", "å†²å‡»éŸ³æ•ˆ_impact", "è‡ªç„¶ç¯å¢ƒ"]:
            print(f"  ğŸ¯ è¯†åˆ«ä¸ºç‰¹æ®ŠçŸ­éŸ³æ•ˆç±»å‹: {short_sfx_category}")
            return short_sfx_category, 0.85
    
        # 2. çŸ­éŸ³æ•ˆå†³ç­–è§„åˆ™
        if is_short_audio:
            # è§„åˆ™1: æ–‡ä»¶åç½®ä¿¡åº¦ > 0.6 æ—¶ï¼Œä¼˜å…ˆä½¿ç”¨æ–‡ä»¶ååˆ†ç±»
            if filename_confidence > 0.6:
                return filename_category, filename_confidence * 0.9
            
            # è§„åˆ™2: ASTç½®ä¿¡åº¦ < 0.5 ä¸”æ–‡ä»¶åæœ‰æ˜ç¡®å…³é”®è¯ï¼Œä½¿ç”¨æ–‡ä»¶ååˆ†ç±»
            if ast_confidence < 0.5 and filename_confidence > 0.3:
                return filename_category, max(ast_confidence, filename_confidence) * 0.85
            
            # è§„åˆ™3: ä¸¤è€…éƒ½ä¸ç¡®å®šï¼Œä½¿ç”¨ä¿å®ˆåˆ†ç±»
            if ast_confidence < 0.4 and filename_confidence < 0.4:
                return self._get_conservative_category(filename_keywords, ast_labels), 0.4
    
        # 3. é»˜è®¤å›é€€åˆ°åŸå§‹èåˆé€»è¾‘
        return self._fuse_decisions(ast_category, ast_confidence, filename_category, filename_confidence)

    def _fuse_decisions(self, ast_category, ast_confidence, filename_category, filename_confidence):
        """èåˆASTå’Œæ–‡ä»¶åçš„å†³ç­–"""
        # 1. å¦‚æœä¸¤è€…ä¸€è‡´ï¼Œå–æœ€é«˜ç½®ä¿¡åº¦
        if ast_category == filename_category:
            return ast_category, max(ast_confidence, filename_confidence)
        
        # 2. è®¡ç®—åŠ æƒç½®ä¿¡åº¦
        weighted_ast = ast_confidence * 0.7  # ASTæƒé‡70%
        weighted_filename = filename_confidence * 0.3  # æ–‡ä»¶åæƒé‡30%
        
        # 3. ç‰¹æ®Šè§„åˆ™ï¼šæ–‡ä»¶ååŒ…å«æ˜ç¡®ä¸“ä¸šæœ¯è¯­æ—¶ï¼Œä¼˜å…ˆè€ƒè™‘æ–‡ä»¶å
        if filename_confidence > 0.7 and ast_confidence < 0.5:
            return filename_category, filename_confidence
        
        # 4. é»˜è®¤è§„åˆ™ï¼šå–åŠ æƒç½®ä¿¡åº¦é«˜çš„
        if weighted_ast > weighted_filename:
            return ast_category, ast_confidence
        else:
            return filename_category, filename_confidence

    def _detect_short_sfx_type(self, filename_keywords, ast_labels, duration):
        """æ£€æµ‹ç‰¹æ®ŠçŸ­éŸ³æ•ˆç±»å‹"""
        # æå–æ‰€æœ‰å…³é”®è¯
        all_keywords = filename_keywords.get("explicit_keywords", []) + \
                      filename_keywords.get("context_keywords", []) + \
                      [label.lower() for label in ast_labels]
        
        all_keywords_str = " ".join(all_keywords).lower()
        
        # 1. æ£€æŸ¥å†²å‡»/å¼ºè°ƒéŸ³æ•ˆ
        impact_keywords = ["click", "tap", "snap", "pop", "thud", "hit", "stinger", "stab", 
                          "ç‚¹å‡»", "è½»è§¦", "å¼¹å“", "çˆ†ç ´", "å¼ºè°ƒ", "ç¬é—´"]
        if any(kw in all_keywords_str for kw in impact_keywords) and duration < 1.0:
            return "è½¬åœºéŸ³æ•ˆ"  # ä½¿ç”¨ç°æœ‰åˆ†ç±»
        
        # 2. æ£€æŸ¥UIäº¤äº’éŸ³æ•ˆ
        ui_keywords = ["button", "beep", "notification", "system", "ui", "ç•Œé¢", "ç³»ç»Ÿ", "æç¤º"]
        if any(kw in all_keywords_str for kw in ui_keywords) and duration < 0.8:
            return "UIäº¤äº’"
        
        # 3. æ£€æŸ¥æè´¨éŸ³æ•ˆï¼ˆçº¸/å¸ƒ/å¡‘æ–™ï¼‰
        material_keywords = ["paper", "cloth", "plastic", "foil", "rustle", "crinkle", 
                            "çº¸", "å¸ƒæ–™", "å¡‘æ–™", "é”¡çº¸", "æ‘©æ“¦", "æ‰æ“"]
        if any(kw in all_keywords_str for kw in material_keywords) and duration < 1.2:
            return "è‡ªç„¶ç¯å¢ƒ"  # æˆ–è€…åˆ›å»º"æè´¨äº¤äº’"ç±»åˆ«ï¼Œä½†ä½¿ç”¨ç°æœ‰
        
        # 4. æ£€æŸ¥è‡ªç„¶çŸ­éŸ³æ•ˆ
        nature_keywords = ["drop", "drip", "splash", "leaf", "bird_chirp", 
                          "æ°´æ»´", "æ»´è½", "æ°´èŠ±", "æ ‘å¶", "é¸Ÿå«"]
        if any(kw in all_keywords_str for kw in nature_keywords) and duration < 0.7:
            return "è‡ªç„¶ç¯å¢ƒ"
        
        return None

    def _get_conservative_category(self, filename_keywords, ast_labels):
        """ä¿å®ˆåˆ†ç±»ç­–ç•¥ï¼šä½ç½®ä¿¡åº¦æ—¶ä½¿ç”¨"""
        # 1. æ£€æŸ¥æ–‡ä»¶åä¸­çš„æ˜ç¡®ç±»åˆ«æŒ‡ç¤º
        explicit_keys = " ".join(filename_keywords.get("explicit_keywords", [])).lower()
        
        if any(kw in explicit_keys for kw in ["ui", "button", "click", "ç•Œé¢", "æŒ‰é’®", "ç‚¹å‡»"]):
            return "UIäº¤äº’"
        if any(kw in explicit_keys for kw in ["impact", "hit", "thud", "å†²å‡»", "æ‰“å‡»", "ç °"]):
            return "è½¬åœºéŸ³æ•ˆ"  # ä½¿ç”¨ç°æœ‰åˆ†ç±»
        if any(kw in explicit_keys for kw in ["paper", "cloth", "material", "çº¸", "å¸ƒ", "æè´¨"]):
            return "è‡ªç„¶ç¯å¢ƒ"
        if any(kw in explicit_keys for kw in ["juicer", "blender", "toilet", "flush", "toothbrush", "æ¦¨æ±", "é©¬æ¡¶", "åˆ·ç‰™"]):
            return "ç”Ÿæ´»å®¶å±…"
        
        # 2. æœ€åå›é€€
        return "æœªåˆ†ç±»ç´ æ"

    def _repair_low_confidence(self, waveform, base_category, confidence, filename_keywords):
        """æ™ºèƒ½ä¿®å¤ä½ç½®ä¿¡åº¦åˆ†ç±»"""
        if confidence < 0.5:
            print(f"  ğŸ”§ ä¿®å¤ä½ç½®ä¿¡åº¦ ({confidence:.2f})...")
            
            # ç­–ç•¥1: ä¼˜å…ˆä½¿ç”¨æ–‡ä»¶åå…³é”®è¯
            filename_category, filename_confidence = self._analyze_filename_for_category(
                filename_keywords, CATEGORY_LIST
            )
            
            if filename_confidence > confidence * 1.5:
                print(f"  âœ… ç”¨æ–‡ä»¶åè¦†ç›–: {base_category}({confidence:.2f}) â†’ {filename_category}({filename_confidence:.2f})")
                return filename_category, filename_confidence
            
            # ç­–ç•¥2: æ£€æŸ¥çŸ­éŸ³æ•ˆç±»å‹
            duration = waveform.shape[1] / 16000  # è®¡ç®—éŸ³é¢‘æ—¶é•¿
            short_sfx_type = self._detect_short_sfx_type(filename_keywords, [base_category], duration)
            if short_sfx_type:
                print(f"  âœ… è¯†åˆ«ä¸ºçŸ­éŸ³æ•ˆç±»å‹: {short_sfx_type}")
                return short_sfx_type, 0.8
            
            # ç­–ç•¥3: ä¿å®ˆå›é€€
            print(f"  âš ï¸ æ— æ³•ä¿®å¤ï¼Œä½¿ç”¨ä¿å®ˆåˆ†ç±»: æœªåˆ†ç±»ç´ æ")
            return "æœªåˆ†ç±»ç´ æ", 0.4
        
        return base_category, confidence

    def _map_to_simple_category(self, ast_labels, target_categories):
        """ç²¾å‡†æ˜ å°„é€»è¾‘"""
        mapping_rules = {
            "è‡ªç„¶ç¯å¢ƒ": ["wave", "rain", "wind", "thunder", "ocean", "bird", "animal", "forest", "water", "nature",
                        "stream", "river", "brook", "creek", "waterfall", "drip", "splash"],
            "åŸå¸‚ç¯å¢ƒ": ["traffic", "car", "bus", "train", "subway", "siren", "crowd", "street", "urban",
                        "construction", "drill", "jackhammer", "horn", "ambulance", "police_siren"],
            "æœºæ¢°è®¾å¤‡": ["engine", "machine", "drill", "saw", "fan", "motor", "tools", "mechanical",
                        "industrial", "factory", "generator", "compressor", "pump", "turbine"],
            "ç”Ÿæ´»å®¶å±…": ["juicer", "blender", "toilet", "flush", "toothbrush", "bathroom", "kitchen", "appliance",
                        "home", "house", "domestic", "appliances", "refrigerator", "microwave"],
            "äººå£°": ["speech", "voice", "talk", "dialog", "narration", "laugh", "cry", "human",
                    "breathing", "footsteps", "clothing", "rustle", "whisper", "sigh", "yawn"],
            "å†·å…µå™¨": ["sword", "blade", "knife", "metal", "clash", "impact", "whoosh", "kungfu", "martial",
                    "samurai", "swordsmen", "katana", "dagger", "spear"],
            "çƒ­å…µå™¨": ["gun", "rifle", "shotgun", "machine_gun", "sniper", "bullet", "explosion", "bomb", "grenade",
                    "firearm", "ammunition", "detonation", "war", "military", "combat"],
            "åŠ¨ç‰©å£°éŸ³": ["dog", "cat", "bird", "animal", "insect", "cricket", "wildlife",
                        "lion", "tiger", "elephant", "monkey", "chimpanzee", "frog"],
            "UIäº¤äº’": ["click", "button", "beep", "notification", "interface", "digital", "ui",
                    "menu", "scroll", "typing", "keyboard", "mouse", "hover"],
            "è½¬åœºéŸ³æ•ˆ": ["transition", "whoosh", "sweep", "riser", "downer", "swish", "effect",
                        "fade", "crossfade", "stinger", "impact_short"],
            "ç”µå½±æ°›å›´": ["atmosphere", "ambient", "mood", "tension", "calm", "emotional",
                        "suspense", "drama", "romance", "horror", "peaceful"],
            "ç‰¹æ®Šæ•ˆæœ": ["sci-fi", "magic", "fantasy", "special", "unusual",
                        "space", "alien", "laser", "energy", "futuristic"]
        }
        
        for category, keywords in mapping_rules.items():
            if any(keyword in label.lower() for label in ast_labels for keyword in keywords):
                if category in target_categories:
                    return category
        
        label_str = " ".join(ast_labels).lower()
        if "water" in label_str or "wave" in label_str or "ocean" in label_str:
            return "è‡ªç„¶ç¯å¢ƒ"
        if "traffic" in label_str or "car" in label_str or "urban" in label_str:
            return "åŸå¸‚ç¯å¢ƒ"
        if "engine" in label_str or "machine" in label_str or "motor" in label_str:
            return "æœºæ¢°è®¾å¤‡"
        if "juice" in label_str or "toilet" in label_str or "tooth" in label_str or "home" in label_str:
            return "ç”Ÿæ´»å®¶å±…"
        
        return "æœªåˆ†ç±»ç´ æ"

    def _analyze_filename_for_category(self, filename_keywords, target_categories):
        """ä»æ–‡ä»¶åå…³é”®è¯åˆ†æåˆ†ç±»"""
        max_confidence = 0.0
        best_category = "æœªåˆ†ç±»ç´ æ"
        
        # 1. æ£€æŸ¥æ˜¾å¼å…³é”®è¯
        for category in target_categories:
            confidence, _ = calculate_keyword_confidence(filename_keywords, category)
            if confidence > max_confidence:
                max_confidence = confidence
                best_category = category
        
        # 2. å¦‚æœç½®ä¿¡åº¦ä½ï¼Œä½¿ç”¨å¯å‘å¼è§„åˆ™
        if max_confidence < 0.3:
            explicit_keys = " ".join(filename_keywords.get("explicit_keywords", [])).lower()
            context_keys = " ".join(filename_keywords.get("context_keywords", [])).lower()
            
            if any(kw in explicit_keys+context_keys for kw in ["rain", "storm", "wind", "thunder", "é›¨", "é£", "é›·"]):
                return "è‡ªç„¶ç¯å¢ƒ", 0.6
            if any(kw in explicit_keys+context_keys for kw in ["traffic", "car", "bus", "train", "äº¤é€š", "æ±½è½¦", "å·´å£«", "ç«è½¦"]):
                return "åŸå¸‚ç¯å¢ƒ", 0.6
            if any(kw in explicit_keys+context_keys for kw in ["juicer", "blender", "toilet", "flush", "toothbrush", 
                                                            "æ¦¨æ±", "æ…æ‹Œæœº", "é©¬æ¡¶", "å†²æ°´", "åˆ·ç‰™"]):
                return "ç”Ÿæ´»å®¶å±…", 0.7
            if any(kw in explicit_keys+context_keys for kw in ["sword", "blade", "metal", "clash", "åˆ€", "å‰‘", "é‡‘å±", "ç¢°æ’"]):
                return "å†·å…µå™¨", 0.7
            if any(kw in explicit_keys+context_keys for kw in ["gun", "rifle", "explosion", "gun", "rifle", "çˆ†ç‚¸", "æªå£°"]):
                return "çƒ­å…µå™¨", 0.7
        
        return best_category, min(max_confidence, 0.85)  # é™åˆ¶æœ€å¤§ç½®ä¿¡åº¦

    def get_semantic_tags(self, waveform, base_category, original_filename, classification_confidence):
        """ç”Ÿæˆå¢å¼ºç‰ˆè¯­ä¹‰æ ‡ç­¾ï¼ˆèåˆæ–‡ä»¶åä¿¡æ¯ï¼‰"""
        acoustic_fingerprint = self._generate_acoustic_fingerprint(waveform)
        
        # 1. ä»æ–‡ä»¶åæå–å…³é”®è¯
        filename_keywords = extract_filename_keywords(original_filename)
        filename_confidence, matched_keywords = calculate_keyword_confidence(filename_keywords, base_category)
        
        # 2. æ„å»ºå¢å¼ºç‰ˆæç¤ºè¯
        filename_context = ""
        if matched_keywords:
            filename_context = f"æ–‡ä»¶åå…³é”®è¯: {', '.join(matched_keywords)}"
        
        # 3. åŠ¨æ€è°ƒæ•´æç¤ºè¯è¯¦ç»†ç¨‹åº¦
        detail_level = "è¯¦ç»†" if classification_confidence > 0.7 else "ä¸­ç­‰"
        focus_area = "é‡ç‚¹æè¿°æ–‡ä»¶åä¸­æåˆ°çš„å…ƒç´ " if filename_confidence > 0.5 else ""
        
        prompt = f"""
ğŸ¯ ä»»åŠ¡è¦æ±‚
ä½ æ˜¯ä¸€ä½ä¸“ä¸šéŸ³æ•ˆè®¾è®¡å¸ˆï¼Œèåˆå£°å­¦ç‰¹å¾ã€åˆ†ç±»ä¿¡æ¯å’ŒåŸå§‹æ–‡ä»¶åå…³é”®è¯ï¼Œç”Ÿæˆï¼š
1. ä¸€æ®µè‡ªç„¶è¯­è¨€çš„åœºæ™¯æè¿° (30å­—ä»¥å†…ï¼Œæœ‰ç”»é¢æ„Ÿï¼Œé‡ç‚¹ä¿¡æ¯æ”¾åœ¨æœ€å‰é¢)
2. 1-6ä¸ªä¸­æ–‡æ ‡ç­¾ (è¦†ç›–: åœºæ™¯åˆ†ç±», åœºæ™¯å†…å®¹, éŸ³æ•ˆç±»å‹, å…·ä½“äº‹ç‰©)
3. 1-3ä¸ªå¯¹åº”çš„è‹±æ–‡æ ‡ç­¾ (ä¸“ä¸šæœ¯è¯­)

æ³¨æ„ï¼š
1.ä½ å¯¹æœ€ç»ˆéŸ³æ•ˆå†…å®¹çš„åˆ¤æ–­ä¸è¦è¿‡æ¸¡ä¾èµ–æ–‡ä»¶åï¼Œå¦‚æœæ–‡ä»¶åæè¿°ä¸æ¸…æ™°ï¼Œæˆ–æ˜¯æ— è§„åˆ™æ–‡ä»¶åï¼Œå°±ç›´æ¥å¿½ç•¥æ–‡ä»¶åä¸åšå‚è€ƒ
2.é•¿åº¦ä½äº1.5ç§’çš„æ–‡ä»¶ TAGé™„åŠ æ ‡æ³¨â€œçŸ­éŸ³æ•ˆâ€
3.å¦‚æœæœ‰èƒ½ç›´æ¥ç¿»è¯‘æˆä¸­æ–‡çš„å•ä¸ªè‹±æ–‡å•è¯çš„æ–‡ä»¶åï¼Œå°±æŠŠå®ƒåŠ å…¥åˆ°TAGé‡Œã€‚

ğŸ“ åŸºç¡€åˆ†ç±»
{base_category} (ç½®ä¿¡åº¦: {classification_confidence:.2f})

ğŸ“„ åŸå§‹æ–‡ä»¶åä¿¡æ¯
åŸå§‹æ–‡ä»¶å: {original_filename}
{filename_context}

ğŸ” ä»»åŠ¡è¦æ±‚
- ç½®ä¿¡åº¦é«˜æ—¶: {detail_level}æè¿°ï¼ŒåŒ…å«ä¸“ä¸šç»†èŠ‚
- {focus_area}
- é¿å…ä¸æ–‡ä»¶åå…³é”®è¯æ˜æ˜¾çŸ›ç›¾çš„æè¿°
- æ‰€æœ‰ä¸­æ–‡æ ‡ç­¾å¿…é¡»æ˜¯çº¯ä¸­æ–‡ï¼Œä¸åŒ…å«ä»»ä½•è‹±æ–‡å­—ç¬¦æˆ–æ•°å­—
- æ ‡ç­¾é•¿åº¦æ§åˆ¶åœ¨2-6ä¸ªä¸­æ–‡å­—ç¬¦

âœ… è¾“å‡ºæ ¼å¼ (ä¸¥æ ¼éµå¾ª)
åœºæ™¯æè¿°: [50å­—ä»¥å†…çš„è‡ªç„¶è¯­è¨€æè¿°]
ä¸­æ–‡æ ‡ç­¾: [è¯1, è¯2, è¯3, è¯4, è¯5, è¯6]
è‹±æ–‡æ ‡ç­¾: [term1, term2, term3]

ğŸ“ ç¤ºä¾‹1
åœºæ™¯æè¿°: æ·±å¤œæš´é›¨å€¾ç›†è€Œä¸‹ï¼Œé›¨ç‚¹ç ¸åœ¨ç«¹å¶ä¸Šå‘å‡ºæ²™æ²™å£°ï¼Œè¿œå¤„ä¼ æ¥éš†éš†é›·å£°
ä¸­æ–‡æ ‡ç­¾: æš´é›¨, ç«¹æ—, é›¨æ»´, é›·å£°, å¤œæ™š, è‡ªç„¶ç¯å¢ƒ, æ°´ä½“, æ°”è±¡, æˆ·å¤–
è‹±æ–‡æ ‡ç­¾: heavy_rain, bamboo_forest, rain_drops

ğŸ“ ç¤ºä¾‹2
åœºæ™¯æè¿°: ä¸¤åŒ¹é©¬åœ¨è‰åŸä¸Šå¥”è·‘
ä¸­æ–‡æ ‡ç­¾: è‡ªç„¶ç¯å¢ƒ, é©¬, åŠ¨ç‰©è‡ªç„¶, é©¬è¹„å£°, è‰åŸ
è‹±æ–‡æ ‡ç­¾: Horses, natural, grassland

ğŸ¯ å½“å‰ä»»åŠ¡
"""
        
        try:
            payload = {
                "model": self.qwen_model, 
                "prompt": prompt, 
                "stream": False,
                "options": {
                    "temperature": 0.2 if classification_confidence > 0.7 else 0.3,
                    "num_ctx": 2048
                }
            }
            response = requests.post(
                self.ollama_api, 
                json=payload, 
                timeout=40
            )
            response.raise_for_status()
            
            content = response.json()['response'].strip()
            
            # æ¸…ç†æ€è€ƒè¿‡ç¨‹
            for marker in ["</think>", "ä»»åŠ¡ç»“æŸ", "æ€è€ƒè¿‡ç¨‹"]:
                if marker in content:
                    content = content.split(marker)[-1].strip()
            
            # è§£æç»“æ„åŒ–è¾“å‡º
            desc_match = re.search(r"åœºæ™¯æè¿°:\s*(.*)", content)
            cn_match = re.search(r"ä¸­æ–‡æ ‡ç­¾:\s*(.*)", content)
            en_match = re.search(r"è‹±æ–‡æ ‡ç­¾:\s*(.*)", content)
            
            # åœºæ™¯æè¿° (50å­—ä»¥å†…)
            description = desc_match.group(1).strip() if desc_match else f"{base_category}ä¸“ä¸šéŸ³æ•ˆç´ æ"
            if len(description) > 50:
                description = description[:50]
            
            # ä¸­æ–‡æ ‡ç­¾ (3-9ä¸ª)
            if cn_match:
                raw_tags_cn = cn_match.group(1).strip()
                tags_cn = [tag.strip() for tag in raw_tags_cn.split(",") if tag.strip()]
                # å½»åº•æ¸…æ´—å¹¶éªŒè¯
                tags_cn = self._clean_chinese_tags(tags_cn)
            else:
                tags_cn = ["æœªçŸ¥éŸ³æ•ˆ"]
            
            # è‹±æ–‡æ ‡ç­¾ (3-9ä¸ªï¼Œå¯¹åº”ä¸­æ–‡)
            if en_match:
                raw_tags_en = en_match.group(1).strip()
                tags_en = [tag.strip() for tag in raw_tags_en.split(",") if tag.strip()]
                tags_en = tags_en[:len(tags_cn)] if len(tags_en) > len(tags_cn) else (tags_en + ["professional_sound"] * (len(tags_cn) - len(tags_en)))[:9]
            else:
                tags_en = ["professional_sound", "film_sound", "sound_design"]
            
            # äºŒæ¬¡æ¸…æ´—
            tags_cn = [clean_chinese_text(tag) for tag in tags_cn if is_valid_chinese(tag)]
            if len(tags_cn) < 3:
                default_tags = ["æœªçŸ¥éŸ³æ•ˆ"]
                for tag in default_tags:
                    if tag not in tags_cn:
                        tags_cn.append(tag)
                    if len(tags_cn) >= 3:
                        break
            
            # è´¨é‡éªŒè¯
            if len(description) < 10:
                description = self._get_fallback_description(base_category)
            
            return description, tags_cn, tags_en
            
        except Exception as e:
            print(f"âš ï¸ Qwen3 API å¤±è´¥: {str(e)}")
            fallback_desc, fallback_cn, fallback_en = self._get_fallback_tags(base_category)
            fallback_cn = [clean_chinese_text(tag) for tag in fallback_cn if is_valid_chinese(tag)]
            return fallback_desc, fallback_cn, fallback_en

    def _clean_chinese_tags(self, tags):
        """å½»åº•æ¸…æ´—ä¸­æ–‡æ ‡ç­¾ï¼Œç¡®ä¿100%ä¸­æ–‡"""
        cleaned_tags = []
        
        for tag in tags:
            # 1. å½»åº•æ¸…æ´—æ–‡æœ¬
            clean_tag = clean_chinese_text(tag)
            
            # 2. éªŒè¯æ˜¯å¦ä¸ºæœ‰æ•ˆä¸­æ–‡
            if is_valid_chinese(clean_tag):
                # 3. é™åˆ¶é•¿åº¦ï¼ˆ2-6ä¸ªä¸­æ–‡å­—ç¬¦ï¼‰
                clean_tag = clean_tag[:6]
                if len(clean_tag) >= 2:
                    cleaned_tags.append(clean_tag)
        
        # 4. ç¡®ä¿è‡³å°‘æœ‰3ä¸ªæ ‡ç­¾
        if len(cleaned_tags) < 3:
            default_tags = ["æœªçŸ¥éŸ³æ•ˆ"]
            for tag in default_tags:
                if tag not in cleaned_tags:
                    cleaned_tags.append(tag)
                if len(cleaned_tags) >= 3:
                    break
        
        # 5. é™åˆ¶æœ€å¤š9ä¸ªæ ‡ç­¾
        return cleaned_tags[:9]

    def _fuse_filename_keywords(self, tags_cn, tags_en, filename_keywords, category):
        """å°†æ–‡ä»¶åå…³é”®è¯æ™ºèƒ½èåˆåˆ°æ ‡ç­¾ä¸­"""
        # 1. æå–å…³é”®æ–‡ä»¶åå…³é”®è¯
        explicit_keys = filename_keywords.get("explicit_keywords", [])
        context_keys = filename_keywords.get("context_keywords", [])
        
        # 2. ä¼˜å…ˆçº§ï¼šä¸“ä¸šç±»å‹å…³é”®è¯ > å…·ä½“å†…å®¹å…³é”®è¯ > é€šç”¨å…³é”®è¯
        all_keys = explicit_keys + context_keys
        
        # 3. ç­›é€‰ä¸åˆ†ç±»ç›¸å…³çš„å…³é”®è¯
        relevant_keys = []
        for key in all_keys:
            # æ£€æŸ¥æ˜¯å¦ä¸å½“å‰åˆ†ç±»ç›¸å…³
            for cat, kw_list in PROFESSIONAL_KEYWORDS.items():
                if cat == category and any(kw.lower() in key.lower() for kw in kw_list):
                    relevant_keys.append(key)
                    break
        
        # 4. å»é‡å’Œæ¸…æ´—
        relevant_keys = list(dict.fromkeys([k.strip() for k in relevant_keys if k.strip()]))
        
        # 5. æ™ºèƒ½èåˆ
        if relevant_keys:
            # å°†æœ€ç›¸å…³çš„2ä¸ªå…³é”®è¯æ’å…¥åˆ°æ ‡ç­¾å‰éƒ¨
            new_tags_cn = relevant_keys[:2] + [t for t in tags_cn if t not in relevant_keys][:7]
            # è‹±æ–‡æ ‡ç­¾ä¿æŒåŸæœ‰é€»è¾‘ï¼Œä½†ç¡®ä¿æ•°é‡åŒ¹é…
            new_tags_en = [self._translate_to_english(k) for k in relevant_keys[:2]] + \
                         [t for t in tags_en if t not in [self._translate_to_english(k) for k in relevant_keys]][:7]
            
            # å»é‡
            new_tags_cn = list(dict.fromkeys(new_tags_cn))[:9]
            new_tags_en = list(dict.fromkeys(new_tags_en))[:9]
            
            return new_tags_cn, new_tags_en
        
        return tags_cn, tags_en

    def _translate_to_english(self, chinese_text):
        """ç®€å•ä¸­è¯‘è‹±ï¼ˆä¸“ä¸šéŸ³æ•ˆæœ¯è¯­ï¼‰"""
        translation_map = {
            # è‡ªç„¶
            "é›¨": "rain", "æš´é›¨": "heavy_rain", "é£": "wind", "é›·": "thunder", "æµ·æµª": "ocean_wave",
            "æ°´": "water", "æºªæµ": "stream", "æ£®æ—": "forest", "é¸Ÿ": "bird", "è™«": "insect",
            
            # åŸå¸‚
            "äº¤é€š": "traffic", "æ±½è½¦": "car", "äººç¾¤": "crowd", "è¡—é“": "street", "åŸå¸‚": "city",
            
            # ç”Ÿæ´»å®¶å±…
            "æ¦¨æ±": "juicer", "æ…æ‹Œæœº": "blender", "é©¬æ¡¶": "toilet", "å†²æ°´": "flush", "åˆ·ç‰™": "brushing_teeth",
            "ç‰™åˆ·": "toothbrush", "å¨æˆ¿": "kitchen", "å«æµ´": "bathroom", "å®¶å±…": "household",
            
            # å†·å…µå™¨
            "åˆ€": "sword", "å‰‘": "sword", "åˆ€å‰‘": "sword", "é‡‘å±": "metal", "ç¢°æ’": "clash",
            "æ‰“å‡»": "impact", "åŠŸå¤«": "kungfu", "æ­¦æœ¯": "martial_arts", "æ­¦å£«": "warrior",
            
            # çƒ­å…µå™¨
            "æª": "gun", "æ­¥æª": "rifle", "æ‰‹æª": "pistol", "çˆ†ç‚¸": "explosion", "ç‚¸å¼¹": "bomb",
            "æ‰‹æ¦´å¼¹": "grenade", "å†›äº‹": "military", "æˆ˜äº‰": "war", "æˆ˜æ–—": "combat",
            
            # ä¸“ä¸šç±»å‹
            "å†²å‡»": "impact", "ä¸Šå‡": "riser", "è½¬åœº": "transition", "è„‰å†²": "pulse", "æ°›å›´": "atmosphere",
            "å¼ºè°ƒ": "stinger", "è¿‡æ¸¡": "crossfade", "ç¯å¢ƒ": "ambience"
        }
        
        # ç²¾ç¡®åŒ¹é…
        if chinese_text in translation_map:
            return translation_map[chinese_text]
        
        # éƒ¨åˆ†åŒ¹é…
        for zh, en in translation_map.items():
            if zh in chinese_text:
                return en
        
        # é»˜è®¤å¤„ç†ï¼šå°å†™+ä¸‹åˆ’çº¿
        return re.sub(r'[^\w\s]', '', chinese_text).lower().replace(' ', '_')

    def _get_fallback_tags(self, base_category):
        """æ™ºèƒ½å›é€€æ–¹æ¡ˆï¼ˆç¡®ä¿ä¸­æ–‡æ ‡ç­¾çº¯å‡€ï¼‰"""
        fallbacks = {
            "è‡ªç„¶ç¯å¢ƒ": (
                "æ·±å¤œæ£®æ—ä¸­ï¼Œå¾®é£å¹è¿‡æ ‘å¶å‘å‡ºæ²™æ²™å£°ï¼Œè¿œå¤„ä¼ æ¥æ½ºæ½ºæµæ°´å’Œè™«é¸£",
                ["æ£®æ—", "å¾®é£", "æ ‘å¶", "æµæ°´", "è™«é¸£", "å¤œæ™š", "è‡ªç„¶ç¯å¢ƒ", "æ°´ä½“", "ç”Ÿç‰©"],
                ["forest", "breeze", "leaves", "stream", "insects", "night", "natural_environment", "water_body", "wildlife"]
            ),
            "åŸå¸‚ç¯å¢ƒ": (
                "ç¹å¿™çš„éƒ½å¸‚è¡—é“ï¼Œæ±½è½¦å–‡å­å£°ã€äººç¾¤äº¤è°ˆå£°å’Œè¿œå¤„çš„æ–½å·¥å£°äº¤ç»‡åœ¨ä¸€èµ·",
                ["è¡—é“", "äº¤é€š", "äººç¾¤", "åŸå¸‚", "è½¦è¾†", "å–‡å­", "æ–½å·¥", "éƒ½å¸‚", "ç¯å¢ƒå£°"],
                ["street", "traffic", "crowd", "city", "vehicles", "horn", "construction", "urban", "ambience"]
            ),
            "æœºæ¢°è®¾å¤‡": (
                "è€æ—§æŸ´æ²¹å‘åŠ¨æœºåœ¨å‚æˆ¿å†…æŒç»­è¿è½¬ï¼Œå‘å‡ºä½æ²‰çš„è½°é¸£å’Œæœºæ¢°é›¶ä»¶çš„è§„å¾‹ç¢°æ’å£°",
                ["å¼•æ“", "æœºæ¢°", "å·¥ä¸š", "æŸ´æ²¹", "è¿è½¬", "è½°é¸£", "å‚æˆ¿", "è®¾å¤‡", "åŠ¨åŠ›ç³»ç»Ÿ"],
                ["engine", "mechanical", "industrial", "diesel", "operation", "roar", "factory", "equipment", "power_system"]
            ),
            "ç”Ÿæ´»å®¶å±…": (
                "æ¸…æ™¨å¨æˆ¿ä¸­ï¼Œæ¦¨æ±æœºå—¡å—¡è¿è½¬ï¼Œæ–°é²œæ°´æœè¢«æ…æ‹Œæˆæ±ï¼Œæ—è¾¹æ°´é¾™å¤´æ»´ç€æ°´ç ",
                ["æ¦¨æ±æœº", "å¨æˆ¿", "æ—©æ™¨", "æ°´æœ", "å®¶ç”¨ç”µå™¨", "ç”Ÿæ´»åœºæ™¯", "æ—¥å¸¸", "å®¶å±…", "æ—©é¤"],
                ["juicer", "kitchen", "morning", "fruit", "household_appliance", "daily_life", "home", "domestic", "breakfast"]
            ),
            "å†·å…µå™¨": (
                "ç«¹æ—ä¸­ä¸¤åæ­¦å£«æŒåˆ€å¯¹å†³ï¼Œé‡‘å±åˆ€èº«ç¢°æ’å‘å‡ºæ¸…è„†å“å£°ï¼Œè¡£è¢‚éšé£é£˜åŠ¨",
                ["åˆ€å‰‘", "ç¢°æ’", "é‡‘å±", "ç«¹æ—", "æ­¦å£«", "å¯¹å†³", "å†·å…µå™¨", "ä¼ ç»Ÿ", "æ­¦ä¾ "],
                ["sword", "clash", "metal", "bamboo_forest", "warrior", "duel", "cold_weapon", "traditional", "martial_arts"]
            ),
            "çƒ­å…µå™¨": (
                "æˆ˜åœºä¸Šçš„M16æ­¥æªè¿å‘å°„å‡»ï¼Œå­å¼¹å‘¼å•¸è€Œè¿‡ï¼Œè¿œå¤„çˆ†ç‚¸å£°éœ‡è€³æ¬²è‹",
                ["æ­¥æª", "å°„å‡»", "æˆ˜åœº", "çˆ†ç‚¸", "å†›äº‹", "æˆ˜äº‰", "M16", "å­å¼¹", "çˆ†ç‚¸å£°"],
                ["rifle", "shooting", "battlefield", "explosion", "military", "war", "m16", "bullet", "explosion_sound"]
            ),
            "äººå£°": (
                "ç´§å¼ çš„å¯†å®¤å¯¹è¯ï¼Œä¸¤äººä½å£°äº¤è°ˆï¼Œå‘¼å¸æ€¥ä¿ƒï¼Œå¶å°”æœ‰è¡£ç‰©æ‘©æ“¦å£°",
                ["å¯¹è¯", "å¯†å®¤", "ç´§å¼ ", "å‘¼å¸", "ä½è¯­", "äººå£°", "æƒ…ç»ª", "éè¯­è¨€", "æ°›å›´"],
                ["dialog", "secret_room", "tension", "breathing", "whisper", "human_voice", "emotion", "non_verbal", "atmosphere"]
            ),
            "åŠ¨ç‰©å£°éŸ³": (
                "æ¸…æ™¨çƒ­å¸¦é›¨æ—ï¼Œå„ç§é¸Ÿç±»é¸£å«ï¼Œæ˜†è™«å—¡å—¡ï¼Œè¿œå¤„æœ‰çŒ´å­çš„å«å£°",
                ["é¸Ÿé¸£", "æ˜†è™«", "çŒ´å­", "é›¨æ—", "æ¸…æ™¨", "é‡ç”ŸåŠ¨ç‰©", "è‡ªç„¶", "ç”Ÿç‰©", "ç”Ÿæ€ç¯å¢ƒ"],
                ["bird_song", "insects", "monkey", "rainforest", "morning", "wildlife", "nature", "creature", "ecosystem"]
            ),
            "UIäº¤äº’": (
                "æœªæ¥ç§‘æŠ€ç•Œé¢ï¼ŒæŒ‰é’®ç‚¹å‡»æ—¶å‘å‡ºæ¸…è„†çš„ç¡®è®¤å£°ï¼Œä¼´éšå¾®å¦™çš„è§¦è§‰åé¦ˆå’Œè§†è§‰æç¤ºéŸ³",
                ["æŒ‰é’®", "ç³»ç»Ÿ", "ç¡®è®¤", "ç•Œé¢", "è§¦è§‰", "æœªæ¥", "ç§‘æŠ€", "åé¦ˆ", "äº¤äº’"],
                ["button", "system", "confirmation", "interface", "haptic", "futuristic", "technology", "feedback", "interaction"]
            ),
            "è½¬åœºéŸ³æ•ˆ": (
                "ç”µå½±åœºæ™¯ä»å®‰é™çš„å®¤å†…åˆ‡æ¢åˆ°ç‹‚é£æš´é›¨çš„æˆ·å¤–ï¼Œä½¿ç”¨ä¸Šå‡éŸ³æ•ˆå’Œé£å£°æ¸å˜è¿‡æ¸¡",
                ["è½¬åœº", "ä¸Šå‡", "é£å£°", "æ¸å˜", "ç”µå½±", "åœºæ™¯", "è¿‡æ¸¡", "åŠ¨æ€", "æ•ˆæœ"],
                ["transition", "riser", "wind", "fade", "cinema", "scene", "crossfade", "dynamic", "effect"]
            ),
            "ç”µå½±æ°›å›´": (
                "æ‚¬ç–‘åœºæ™¯çš„ç´§å¼ æ°›å›´é“ºå«ï¼Œä½é¢‘æŒç»­éŸ³æ•ˆé…åˆç»†å¾®çš„å¿ƒè·³å£°ï¼Œè¥é€ ä¸å®‰æƒ…ç»ª",
                ["æ°›å›´", "æ‚¬ç–‘", "é“ºå«", "ä½é¢‘", "å¿ƒè·³", "ç´§å¼ ", "æƒ…ç»ª", "å¿ƒç†", "ç”µå½±"],
                ["atmosphere", "suspense", "buildup", "low_frequency", "heartbeat", "tension", "emotion", "psychological", "cinema"]
            ),
            "ç‰¹æ®Šæ•ˆæœ": (
                "ç§‘å¹»ç©ºé—´æ‰­æ›²æ•ˆæœï¼Œèƒ½é‡æ³¢åŠ¨äº§ç”Ÿå—¡å—¡å£°ï¼Œä¼´éšç²’å­æ¶ˆæ•£çš„å˜¶å˜¶å£°",
                ["ç§‘å¹»", "ç©ºé—´", "èƒ½é‡", "æ‰­æ›²", "ç²’å­", "æœªæ¥", "è¶…è‡ªç„¶", "å˜å½¢", "ç‰¹æ•ˆ"],
                ["sci_fi", "space", "energy", "distortion", "particles", "futuristic", "supernatural", "morph", "special_effect"]
            ),
            "æœªåˆ†ç±»ç´ æ": (
                "ä¸“ä¸šéŸ³æ•ˆç´ æï¼Œé€‚ç”¨äºå¤šç§å½±è§†å’Œæ¸¸æˆåœºæ™¯ï¼Œå…·æœ‰ç‹¬ç‰¹çš„å£°å­¦ç‰¹å¾",
                ["ä¸“ä¸š", "éŸ³æ•ˆ", "ç´ æ", "å½±è§†", "æ¸¸æˆ", "é€šç”¨", "è®¾è®¡", "åˆ›æ„", "èµ„æº"],
                ["professional", "sound_effect", "material", "film", "game", "versatile", "design", "creative", "resource"]
            )
        }
        
        # ç¡®ä¿å›é€€æ ‡ç­¾æ˜¯çº¯ä¸­æ–‡
        if base_category in fallbacks:
            desc, cn_tags, en_tags = fallbacks[base_category]
            return desc, cn_tags, en_tags
        else:
            return (
                f"{base_category}ä¸“ä¸šéŸ³æ•ˆåœºæ™¯æè¿°",
                ["ä¸“ä¸šéŸ³æ•ˆ", "å½±è§†ç´ æ", "éŸ³æ•ˆè®¾è®¡", "åˆ›æ„å£°éŸ³", "ä¸“ä¸šåˆ¶ä½œ"],
                ["professional_sound", "film_material", "sound_design", "creative_audio", "professional_production"]
            )

    def _get_fallback_description(self, base_category):
        """å›é€€æè¿°"""
        descriptions = {
            "è‡ªç„¶ç¯å¢ƒ": "æ·±å¤œæ£®æ—ä¸­ï¼Œå¾®é£å¹è¿‡æ ‘å¶å‘å‡ºæ²™æ²™å£°ï¼Œè¿œå¤„ä¼ æ¥æ½ºæ½ºæµæ°´å’Œè™«é¸£",
            "åŸå¸‚ç¯å¢ƒ": "ç¹å¿™çš„éƒ½å¸‚è¡—é“ï¼Œæ±½è½¦å–‡å­å£°ã€äººç¾¤äº¤è°ˆå£°å’Œè¿œå¤„çš„æ–½å·¥å£°äº¤ç»‡åœ¨ä¸€èµ·",
            "æœºæ¢°è®¾å¤‡": "è€æ—§æŸ´æ²¹å‘åŠ¨æœºåœ¨å‚æˆ¿å†…æŒç»­è¿è½¬ï¼Œå‘å‡ºä½æ²‰çš„è½°é¸£å’Œæœºæ¢°é›¶ä»¶çš„è§„å¾‹ç¢°æ’å£°",
            "ç”Ÿæ´»å®¶å±…": "æ¸…æ™¨å¨æˆ¿ä¸­ï¼Œæ¦¨æ±æœºå—¡å—¡è¿è½¬ï¼Œæ–°é²œæ°´æœè¢«æ…æ‹Œæˆæ±ï¼Œæ—è¾¹æ°´é¾™å¤´æ»´ç€æ°´ç ",
            "å†·å…µå™¨": "ç«¹æ—ä¸­ä¸¤åæ­¦å£«æŒåˆ€å¯¹å†³ï¼Œé‡‘å±åˆ€èº«ç¢°æ’å‘å‡ºæ¸…è„†å“å£°ï¼Œè¡£è¢‚éšé£é£˜åŠ¨",
            "çƒ­å…µå™¨": "æˆ˜åœºä¸Šçš„M16æ­¥æªè¿å‘å°„å‡»ï¼Œå­å¼¹å‘¼å•¸è€Œè¿‡ï¼Œè¿œå¤„çˆ†ç‚¸å£°éœ‡è€³æ¬²è‹",
            "äººå£°": "ç´§å¼ çš„å¯†å®¤å¯¹è¯ï¼Œä¸¤äººä½å£°äº¤è°ˆï¼Œå‘¼å¸æ€¥ä¿ƒï¼Œå¶å°”æœ‰è¡£ç‰©æ‘©æ“¦å£°"
        }
        return descriptions.get(base_category, f"{base_category}ä¸“ä¸šéŸ³æ•ˆåœºæ™¯")

def main():
    start_time = time.time()  # è®°å½•å¼€å§‹æ—¶é—´
    
    # æ£€æŸ¥ç›®æ ‡ç›®å½•
    os.makedirs(TARGET_DIR, exist_ok=True)
    
    # åˆå§‹åŒ–å¼•æ“
    engine = AIEngine(AST_MODEL, HF_CACHE_DIR, True, OLLAMA_API, QWEN_MODEL)
    
    # åŠ è½½æ•°æ®åº“
    db_data = []
    if os.path.exists(JSON_DB_PATH):
        try:
            with open(JSON_DB_PATH, 'r', encoding='utf-8') as f:
                db_data = json.load(f)
            print(f"âœ… å·²åŠ è½½ {len(db_data)} ä¸ªå·²å¤„ç†æ–‡ä»¶è®°å½•")
        except Exception as e:
            print(f"âš ï¸ è¯»å–å†å²è®°å½•å¤±è´¥: {str(e)}")
    processed_hashes = {item['md5'] for item in db_data}

    # files = [
    #     os.path.join(root, f)
    #     for root, _, filenames in os.walk(SOURCE_DIR)
    #     for f in filenames if f.lower().endswith(audio_exts)
    # ]

    # æ”¶é›†æ–‡ä»¶
    files = collect_audio_files(SOURCE_DIRS, audio_exts)

    print(f"\n{'='*60}")
    print(f"ğŸ¯ ä»»åŠ¡é…ç½®:")
    print(f"   æºç›®å½•: {SOURCE_DIRS}")
    print(f"   ç›®æ ‡ç›®å½•: {TARGET_DIR}")
    print(f"   æ–‡ä»¶æ€»æ•°: {len(files)}")
    print(f"   å·²å¤„ç†: {len(processed_hashes)}")
    print(f"   å¾…å¤„ç†: {len(files) - len(processed_hashes)}")
    print(f"   æ™ºèƒ½èåˆ: åŸå§‹æ–‡ä»¶å + AIåˆ†æ")
    print(f"{'='*60}")
    
    # å¤„ç†æ–‡ä»¶
    processed_count = 0
    for f_path in tqdm(files, desc="æ•´ç†ä¸­", ascii=True):
        try:
            f_hash = get_file_md5(f_path)
            if f_hash in processed_hashes:
                continue
            
            # è·å–åŸå§‹æ–‡ä»¶å
            original_filename = os.path.basename(f_path)
            
            waveform = preprocess_audio(f_path)
            
            # 1. èåˆåˆ†ç±»ï¼ˆåŒ…å«æ–‡ä»¶ååˆ†æï¼‰
            initial_cat, classification_confidence = engine.classify_audio(
                waveform, CATEGORY_LIST, original_filename
            )
            
            # 2. ç½®ä¿¡åº¦<0.5æ—¶å¼ºåˆ¶é‡å®šå‘åˆ°æœªåˆ†ç±»ç›®å½•
            original_cat_for_stats = initial_cat  # ä¿ç•™åŸå§‹åˆ†ç±»ç”¨äºç»Ÿè®¡
            if classification_confidence < 0.5 and initial_cat != "æœªåˆ†ç±»ç´ æ":
                print(f"  âš ï¸ ä½ç½®ä¿¡åº¦ ({classification_confidence:.2f})ï¼Œå¼ºåˆ¶é‡å®šå‘åˆ°: æœªåˆ†ç±»ç´ æ")
                initial_cat = "æœªåˆ†ç±»ç´ æ"
            
            # 3. ç”Ÿæˆå¢å¼ºæ ‡ç­¾ï¼ˆèåˆæ–‡ä»¶åä¿¡æ¯ï¼‰
            description, tags_cn, tags_en = engine.get_semantic_tags(
                waveform, initial_cat, original_filename, classification_confidence
            )
            
            # 4. ç”Ÿæˆæ–‡ä»¶å
            file_ext = os.path.splitext(f_path)[1].lower()
            new_name = generate_readable_filename(description, file_ext, f_hash)
            
            # 5. ä¿å­˜æ–‡ä»¶ - ä½¿ç”¨ä¿®æ­£åçš„åˆ†ç±»
            dest_dir = os.path.join(TARGET_DIR, initial_cat)
            os.makedirs(dest_dir, exist_ok=True)
            final_path = os.path.join(dest_dir, new_name)
            
            # é¿å…æ–‡ä»¶åå†²çª
            counter = 1
            while os.path.exists(final_path) and counter < 100:
                name_body = new_name.rsplit('.', 1)[0]
                name_ext = new_name.rsplit('.', 1)[1] if '.' in new_name else ''
                new_name = f"{name_body}-{counter}.{name_ext}" if name_ext else f"{name_body}-{counter}"
                final_path = os.path.join(dest_dir, new_name)
                counter += 1
            
            shutil.copy2(f_path, final_path)
            
            # 6. è®¡ç®—ç›¸å¯¹è·¯å¾„
            relative_path = os.path.relpath(final_path, TARGET_DIR).replace('\\', '/')
            
            # 7. è®°å½•åˆ°æ•°æ®åº“
            db_data.append({
                "md5": f_hash,
                "filename": new_name,
                "full_path": final_path,
                "relative_path": relative_path,
                "category": initial_cat,
                "original_category": original_cat_for_stats,  # ä¿ç•™åŸå§‹åˆ†ç±»
                "classification_confidence": float(classification_confidence),
                "tags_cn": tags_cn,
                "tags_en": tags_en,
                "description": description,
                "original_filename": original_filename,  # ä¿å­˜åŸå§‹æ–‡ä»¶å
                "filename_keywords": extract_filename_keywords(original_filename),  # ä¿å­˜æå–çš„å…³é”®è¯
                "original_path": f_path,
                "created_at": datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")
            })
            processed_hashes.add(f_hash)
            processed_count += 1
            
            # 8. ä¿å­˜è¿›åº¦
            if processed_count % 5 == 0:
                with open(JSON_DB_PATH, 'w', encoding='utf-8') as f:
                    json.dump(db_data, f, ensure_ascii=False, indent=2)
                print(f"\nğŸ’¾ å·²ä¿å­˜è¿›åº¦: {processed_count} ä¸ªæ–‡ä»¶")
            
            # 9. æ‰“å°è¯¦ç»†ä¿¡æ¯
            print(f"\nâœ… å¤„ç†æˆåŠŸ: {original_filename}")
            print(f"   æœ€ç»ˆåˆ†ç±»: {initial_cat} (ç½®ä¿¡åº¦: {classification_confidence:.2f})")
            print(f"   åŸå§‹åˆ†ç±»: {original_cat_for_stats}")
            print(f"   åœºæ™¯æè¿°: {description}")
            print(f"   ä¸­æ–‡æ ‡ç­¾: {', '.join(tags_cn)}")
            print(f"   è‹±æ–‡æ ‡ç­¾: {', '.join(tags_en)}")
            print(f"   ç›¸å¯¹è·¯å¾„: {relative_path}")
            print(f"   ä¿å­˜ä¸º: {new_name}")
            
        except Exception as e:
            print(f"\nâŒ å¤„ç†å¤±è´¥ {os.path.basename(f_path)}: {str(e)}")
            # é”™è¯¯æ–‡ä»¶ä¹Ÿæ”¾å…¥æœªåˆ†ç±»ç›®å½•
            try:
                # ç”Ÿæˆå›é€€æ–‡ä»¶å
                file_ext = os.path.splitext(f_path)[1].lower()
                file_hash = get_file_md5(f_path)
                new_name = f"error_{file_hash[:6]}{file_ext}"
                
                # ä¿å­˜åˆ°æœªåˆ†ç±»ç›®å½•
                dest_dir = os.path.join(TARGET_DIR, "æœªåˆ†ç±»ç´ æ")
                os.makedirs(dest_dir, exist_ok=True)
                final_path = os.path.join(dest_dir, new_name)
                
                shutil.copy2(f_path, final_path)
                
                # è®¡ç®—ç›¸å¯¹è·¯å¾„
                relative_path = os.path.relpath(final_path, TARGET_DIR).replace('\\', '/')
                
                # è®°å½•åˆ°æ•°æ®åº“
                db_data.append({
                    "md5": file_hash,
                    "filename": new_name,
                    "full_path": final_path,
                    "relative_path": relative_path,
                    "category": "æœªåˆ†ç±»ç´ æ",
                    "classification_confidence": 0.0,
                    "tags_cn": ["é”™è¯¯", "å¤„ç†å¤±è´¥"],
                    "tags_en": ["error", "processing_failed"],
                    "description": f"å¤„ç†å¤±è´¥: {str(e)}",
                    "original_filename": os.path.basename(f_path),
                    "original_path": f_path,
                    "created_at": datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")
                })
                processed_hashes.add(file_hash)
                processed_count += 1
                
                print(f"  ğŸ›¡ï¸ é”™è¯¯æ–‡ä»¶å·²ä¿å­˜åˆ°: æœªåˆ†ç±»ç´ æ/{new_name}")
            except Exception as backup_e:
                print(f"  âŒ é”™è¯¯å¤„ç†ä¹Ÿå¤±è´¥: {str(backup_e)}")
    
    # æœ€ç»ˆä¿å­˜
    with open(JSON_DB_PATH, 'w', encoding='utf-8') as f:
        json.dump(db_data, f, ensure_ascii=False, indent=2)
    
    # ä»»åŠ¡æ‘˜è¦
    print(f"\n{'='*60}")
    print("ğŸ‰ ä»»åŠ¡å®Œæˆæ‘˜è¦")
    print(f"{'='*60}")
    print(f"ğŸ“ æºç›®å½•: {SOURCE_DIRS}")
    print(f"ğŸ“‚ ç›®æ ‡ç›®å½•: {TARGET_DIR}")
    print(f"âœ… æ–°å¢å¤„ç†: {processed_count} ä¸ªæ–‡ä»¶")
    print(f"ğŸ“Š æ€»è®°å½•æ•°: {len(db_data)}")
    
    # åˆ†ç±»ç»Ÿè®¡
    category_stats = {}
    low_conf_stats = {}
    for item in db_data:
        category_stats[item["category"]] = category_stats.get(item["category"], 0) + 1
        
        # ä½ç½®ä¿¡åº¦ç»Ÿè®¡
        if item["classification_confidence"] < 0.5:
            low_conf_stats[item["category"]] = low_conf_stats.get(item["category"], 0) + 1
    
    print("\nğŸ“ˆ åˆ†ç±»ç»Ÿè®¡:")
    for cat, count in sorted(category_stats.items(), key=lambda x: x[1], reverse=True):
        low_count = low_conf_stats.get(cat, 0)
        if low_count > 0:
            print(f"   {cat}: {count} ä¸ªæ–‡ä»¶ ({low_count} ä¸ªä½ç½®ä¿¡åº¦)")
        else:
            print(f"   {cat}: {count} ä¸ªæ–‡ä»¶")
    
    # ç½®ä¿¡åº¦ç»Ÿè®¡
    avg_confidence = sum(item["classification_confidence"] for item in db_data) / len(db_data) if db_data else 0
    high_conf_files = [f for f in db_data if f["classification_confidence"] > 0.8]
    medium_conf_files = [f for f in db_data if 0.6 <= f["classification_confidence"] <= 0.8]
    low_conf_files = [f for f in db_data if f["classification_confidence"] < 0.6]
    
    print(f"\nğŸ“Š ç½®ä¿¡åº¦ç»Ÿè®¡:")
    print(f"   å¹³å‡ç½®ä¿¡åº¦: {avg_confidence:.3f}")
    print(f"   é«˜ç½®ä¿¡åº¦(>0.8): {len(high_conf_files)} ä¸ªæ–‡ä»¶ ({len(high_conf_files)/len(db_data)*100:.1f}%)")
    print(f"   ä¸­ç½®ä¿¡åº¦(0.6-0.8): {len(medium_conf_files)} ä¸ªæ–‡ä»¶ ({len(medium_conf_files)/len(db_data)*100:.1f}%)")
    print(f"   ä½ç½®ä¿¡åº¦(<0.6): {len(low_conf_files)} ä¸ªæ–‡ä»¶ ({len(low_conf_files)/len(db_data)*100:.1f}%)")
    print(f"   æœªåˆ†ç±»ç´ æç›®å½•: {category_stats.get('æœªåˆ†ç±»ç´ æ', 0)} ä¸ªæ–‡ä»¶")
    
    # æ€»è€—æ—¶è®¡ç®—
    end_time = time.time()
    elapsed_seconds = end_time - start_time
    
    # è½¬æ¢ä¸ºåˆ†ç§’æ ¼å¼
    minutes = int(elapsed_seconds // 60)
    seconds = int(elapsed_seconds % 60)
    
    # æ ¼å¼åŒ–è€—æ—¶å­—ç¬¦ä¸²
    if minutes > 0:
        duration_str = f"{minutes}åˆ†{seconds}ç§’"
    else:
        duration_str = f"{seconds}ç§’"
    
    # æ˜¾ç¤ºæ€»è€—æ—¶
    print(f"\nâ±ï¸  æ€»è€—æ—¶: {duration_str} ({elapsed_seconds:.1f}ç§’)")
    print(f"âš¡ å¹³å‡å¤„ç†é€Ÿåº¦: {elapsed_seconds/max(processed_count,1):.2f}ç§’/æ–‡ä»¶")
    
    print(f"\nğŸ’¡ æ™ºèƒ½èåˆæç¤º: åŸå§‹æ–‡ä»¶åå…³é”®è¯ä¸AIåˆ†æç»“æœå·²å¹³è¡¡èåˆ")
    print("âœ¨ ç³»ç»Ÿèµ„æºå·²é‡Šæ”¾ï¼Œ5ç§’åè‡ªåŠ¨é€€å‡º...")
    time.sleep(5)

if __name__ == "__main__":
    main()